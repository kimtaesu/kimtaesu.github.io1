<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Linear Regression - I</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/metal/2016-03-30-ml-ng-linear-regression/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*kimtaesu</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- kimtaesu instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/cuda/">CUDA</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/metal/">Machine Learning</a>
          
        
          
          <a class="page-link" href="/portfolio/">Gallery</a>
          
        
          
          <a class="page-link" href="/self_improvement/">Self-Improve</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Linear Regression - I</h1>
    <p class="post-meta">March 30, 2016</p>
  </header>

  <article class="post-content">
    <h1 id="linear-regression">Linear Regression</h1>

<h2 id="변수-하나one-variable에-대한-linear-regression">변수 하나(one variable)에 대한 Linear Regression</h2>

<h3 id="모델">모델</h3>
<p>앞서 설명한대로 <strong>Regression</strong> 문제는 연속적인 결화를 도출하는 함수에 대한 예측을 하는 문제이다. 여기서 살펴볼 문제는 변수 하나에 대한 Regression이므로 Univariate linear regression이라고도 불린다.</p>

<p>단일 변수에 대한 Linear Regression은 하나의 입력값에 대한 하나의 출력을 특징인 예측을 하는 경우에 유용하다. 또한 Supervised Learning을 사용하는데, 이는 우리가 특정 입력에 대하여 어떤 결과가 나와야 하는지 알고 있기 때문이다.</p>

<h3 id="hypothesis-function">Hypothesis function</h3>
<p>입력에 대한 출력은 다음과 같이 나타낼 수 있다.
<script type="math/tex">h_\theta (x) = \theta_1 + \theta_1 x</script></p>

<p>주어진 $ h_\theta (x)$에 대하여 $\theta_0$과 $\theta_1$를 주어졌을때 우리는 출력값인 $y$를 알 수 있을 것이다. 즉, 우리는 여기서 $h_\theta (x)$ 함수를 만들 것인데, 그것은 우리가 가진 입력 데이터(x)를 우리가 가진 출력 데이터(y) 간의 상관관계를 표현해 줄 것이다.</p>

<p>여기서 $x$는 입력 데이터 변수이며, $\theta$는 feature이다.</p>

<p>예를 들어, 다음과 같은 데이터가 주어졌다고 하자.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">x:input</th>
      <th style="text-align: center">y:output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">4</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">7</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">7</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">8</td>
    </tr>
  </tbody>
</table>

<p>여기서 임의로 $h_\theta(x)$ 함수에서 $h_0 = 2$이고 $h_1 = 2$라고 가정(hypothesis)해보자. 이런 경우에 대하여 hypothesis function은 다음과 같다.</p>

<script type="math/tex; mode=display">h_\theta(x) = 2 + 2x</script>

<p>그러면 입력 값 1에 대하여, $h_\theta(1)=4$가 되는데 이는 우리가 알고 있는 값보다 3이 적은 값이다. 이렇게 hypothesis function에 의해서 예측한 결과는 오차가 생기게 되는데, 이를 표현하는 방법을 Cost Function이라고 한다.</p>

<h3 id="cost-function">Cost Function</h3>
<p>hypothesis function의 정밀함은 <em>cost function</em>을 통해 알 수 있다. 이것은 입력에 대한 hypotheis function의 결과와 실제 결과 값 간의 오차의 합의 평균(MSE: <a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Square Error</a>)이다. 간단히 분산이라고 생각해도 되는데 약간 차이는 있다.</p>

<p>Cost function은 다음과 같이 표현할 수 있다.</p>

<p><script type="math/tex">J(\theta _0, \theta _1) = \frac {1} {2m} \sum^m_{i=1} ((h_\theta (x)) - y(i))^2</script>
여기서 m은 데이터의 갯수이다.</p>

<p>이 식은 기본적인 MSE 공식에서 $n$이 아니라 $2m$을 사용하였는데, 이는 나중에 사용할 gradient descedent 연산서 간단히 제거가 되기 때문이다.</p>

<p>이제 여기서 우리가 원래 찾으려고 하는 답은 <code class="highlighter-rouge">최적의 hypothesis function</code>은 결국 <code class="highlighter-rouge">오차가 가장 작은 함수</code>를 의미한다는 것을 알 수 있따. 따라서 $J(\theta _0, \theta _1)$를 최소화 한 hypothesis function이 우리가 찾아야 하는 답이다.</p>

<h3 id="gradient-descent">Gradient Descent</h3>
<p>이제 어떻게 하면 hypothesis function의 정확도를 스스로 높일 수 있는지 알아보자. 앞서 우리는 hypothesis function과 그것의 정확도를 평가할 수 있는 cost function에 대해서 알아보았다. 그리고 함수의 최저값을 구하는데는 미분을 사용할 것이고, 여기서 <em>gradient</em>를 사용할 것이다.</p>

<p>이제 우리는 hypothesis function (= $h_\theta (x) = \theta _0 + \theta _1 (x)$)에서 $\theta _0$과 $\theta _1$에 집중할 것이다. 그리고 그 둘의 상관관계에 논할 것이다. $\theta _0$를 $x$축에 $\theta _1$를 $y$축에 두고, cost function의 값을 $z$ 축에 두었을때 3차원의 굴곡진 평면을 생각할 수 있다. 그러면 그 평면에서 가장 아래에 있는 점의 위치가 우리가 찾고자 하는 <strong>minimized cost function</strong>의 위치일 것이다.</p>

<p>이 최저점을 찾는 것은 결국 cost function에 대하여 <code class="highlighter-rouge">편미분</code>을 하는 것이며, cost function의 평면 상 임의의 지점에서 시작해서 미분을 통해 얻은 기울기 벡터값에 따라 이동해가면서 최저점의 위치를 찾는다. 이때 움직이는 길이의 크기를 <code class="highlighter-rouge">learning rate</code>라고 하고 $\alpha$로 나타낸다.</p>

<p>Gradient descent 식은 다음과 같다.</p>

<p><script type="math/tex">\theta _j = \theta _j + \alpha \frac {\delta} {\delta \theta _j} J(\theta _0, \theta _1)</script> for $j = 0, j = 1$</p>

<p>위에서 계산하는 값이 수렴하도록 해야한다.</p>

<h3 id="gradient-descent의-linear-regression-적용">Gradient Descent의 Linear Regression 적용</h3>
<p>결론적으로 cost function의 gradient를 갖고 어떻게 최적의 hypothesis function을 얻기 위해선 오차를 더 적게 gradient에 의한 결과를 cost function에 반영해야 한다.</p>

<p>따라서 수렴할때까지 다음의 과정을 계속한다.</p>

<script type="math/tex; mode=display">\theta _0 = \theta _0 - \alpha \frac {1} {m} \sum ^m _{i=1} (h _\theta (x(i)) - y(i)) x _0</script>

<script type="math/tex; mode=display">\theta _1 = \theta _1 - \alpha \frac {1} {m} \sum ^m _{i=1} (h _\theta (x(i)) - y(i)) x _1</script>

<p>여기서 m은 학습할 데이터의 수이며, $\theta _0$와 $\theta _1$은 hypothesis function의 상수와 파라미터이며, $x(i)$, $y(i)$는 주어진 학습 데이터이다.</p>

<p>결론적으로 위 과정(gradient descent)을 반복할수록 우리의 hypotheis function은 오차가 적어지고 정확해져갈 것이다.</p>

<h2 id="여러개의-변수multiple-variable에-대한-linear-regression">여러개의 변수(Multiple Variable)에 대한 Linear Regression</h2>

<h3 id="모델-1">모델</h3>
<p>앞서 설명한 Linear Regression은 단일 변수(x)에 대한 regression으로서 1차 hypothesis function을 찾는 것에 대해 알아보았다. 그렇다면 여러개의 변수가 주어질 때에는 어떻게 하는 것인지 알아보기로 하자.</p>

<p>예를 들어, 집의 가격과 집의 크기 간의 관계를 살펴보았던 과거의 경우를 살펴보자.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">X</th>
      <th style="text-align: center">Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">집의 크기</td>
      <td style="text-align: center">집의 가격</td>
    </tr>
  </tbody>
</table>

<p>하지만 집의 가격에는 집의 크기 뿐만 아니라 다른 것들도 영향을 미칠 수 있다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center" colspan="2">X</th>
      <th style="text-align: center;">Y</th>
    </tr>
  </thead>
  <tr>
    <td style="text-align: center;">$x _1$</td>
    <td style="text-align: center;">집의 크기</td>
    <td style="text-align: center;" rowspan="4">집의 가격</td>
  </tr>
  <tr>
    <td style="text-align: center;">$x _2$</td>
    <td style="text-align: center;">침실 개수</td>
    
  </tr>
  <tr>
    <td style="text-align: center;">$x _3$</td>
    <td style="text-align: center;">층수</td>
    
  </tr>
  <tr>
    <td style="text-align: center;">$x _4$</td>
    <td style="text-align: center;">건축 연도</td>
  </tr>
</table>

<p>이렇게 여러개의 특징들이 결과 값에 영향을 미치는 경우를 <strong>Multi-variable Linear Regression</strong>이라고 한다.
그러면 앞에서 살펴본 hypothesis function, cost function, gradient descendent 는 어떻게 변하는지 살펴보자.</p>

<h3 id="hypothesis-function-1">Hypothesis function</h3>

<h4 id="single-variable">Single Variable</h4>
<p>$ h_\theta (x) = \theta_1 x_0 + \theta_1 x $</p>

<h4 id="multi-variable-n4">Multi-Variable $(n=4)$</h4>
<p>$ h_\theta (x) = \theta_1 x_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4 $</p>

<p>통상적으로 $x_0$은 표기에서 생략되는데 $x_0 = 1$이다. $x_0$가 있음으로 인해 feature vector ($[\theta _0, \theta _1, \theta _2, …])의 index가 0에서 시작하는 것처럼 입력 변수의 index도 0부터 시작할 수 있다. 이는 프로그래밍을 하는데 있어서 보다 generic한 코딩을 할 수 있게 해주어 코드를 간단하게 만들 수 있게 해준다.</p>

<h3 id="cost-function-1">Cost Function</h3>

<h4 id="single-variable-1">Single Variable</h4>

<p>$J(\theta _0, \theta _1) = \frac {1} {2m} \sum ^m _{i=1} ((h _\theta (x)) - y(i))^2$</p>

<h4 id="multi-variable-n4-1">Multi-Variable $(n=4)$</h4>

<p>$J(\theta) = \frac {1} {2m} \sum^m_{i=1} ((h_\theta (x^{(i)})) - y^{(i)})^2 \enspace where \enspace J(\theta) = J(\theta _0, \theta _1, …, \theta _n)$</p>

<p>얼핏 보기에 식에는 큰 차이가 없어 보이지만, hypothesis function이 바뀌었다는 점을 염두에 두자.</p>

<h3 id="gradient-descent-1">Gradient Descent</h3>

<h4 id="single-variable-2">Single Variable</h4>

<p>$\theta _j = \theta _j - \alpha \frac {1} {m} \sum ^m _{i=1} (h _\theta (x(i)) - y(i)) x _j(i)$</p>

<p>$simultaneously \enspace update \enspace \theta _0, \theta _1 \enspace for \enspace j = 0, j = 1$</p>

<h4 id="multi-variable-n4-2">Multi-Variable $(n=4)$</h4>

<p>$\theta _j = \theta _j - \alpha \frac {1} {m} \sum ^m _{i=1} (h _\theta (x(i)) - y(i)) x _j(i)$</p>

<p>$simultaneously \enspace update \enspace \theta \enspace for \enspace j = 0, 1, …, n$</p>

<p>여기서도 hypothesis가 변경되어서, 편미분을 수행해야하는 대상이 늘어났다는 점을 염두에 두어야 한다. 앞에서 다루었지만 여기서 $\frac {1} {m} \sum ^m _{i=1} (h _\theta (x(i)) - y(i)) x _j(i) = \frac {\delta} {\delta \theta _j} J(\theta)$ 임을 기억하자.</p>

<p>지금까지 Linear Regression의 수학적인 바탕을 살펴보았다. 편미분을 배웠다면 쉽게 상상할 수 있는 개념이다. 다음에는 Linear Regression을 하는데 있어서 고려해야 할 사항들을 다룰 것이다.</p>

  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://localhost:4000/metal/2016-03-30-ml-ng-linear-regression/%20via%20&#64;&hashtags=Machine Learning,Linear Regression,Coursera,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/metal/2016-03-30-ml-ng-linear-regression/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://localhost:4000/metal/2016-03-30-ml-ng-linear-regression/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/metal/2016-03-30-ml-ng-linear-regression/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
