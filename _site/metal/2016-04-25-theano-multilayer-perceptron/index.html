<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Theano를 통한 Multilayer Perceptrion</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/metal/2016-04-25-theano-multilayer-perceptron/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*kimtaesu</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- kimtaesu instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/android">Android</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/kotlin">Kotlin</a>
          
        
          
        
          
          <a class="page-link" href="/python">Python</a>
          
        
          
          <a class="page-link" href="/self-improvement/">Self-Improve</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Theano를 통한 Multilayer Perceptrion</h1>
    <p class="post-meta">April 25, 2016</p>
  </header>

  <article class="post-content">
    <p>앞서 MNIST 예제에서 사용한 Neuarl Network Model에서 사용한 모델은 matrix 연산을 기반으로 machine learning을 한 것으로 아직 deep learning이라고 하기에는 부족다. 이번 에제는 하나의 hidden layer를 갖는 MLP(Multi-Layer Perceptron)을 이용하여 Machine Learning을 해보도록 하자. 여기서부터가 비로소 ANN(Aritificial Neural Network) 또는 Deep Learning이라고 부른다.</p>

<h1 id="model">Model</h1>

<p>MLP를 표현하는 방법은 크게 두 가지 방법이 있는데, 하나는 graph를 사용한 것이고 하나는 수학적인 표현으로 나타내는 것이다.</p>

<div class="img_row center">
<img src="/images/201604/mlp.png" />
<img class="margin-top: auto; margin-bottom: auto;" src="/images/201604/3532b34fba1f5111baf8f41e241768fcafa23540.png" />
</div>
<div class="col three caption">
hidden layer가 하나인 경우에 대하여 MLP를 표현한 예. 하나의 layer가 진행할 때마다 하나의 polynomial equation을 한다고 볼 수 있다.
</div>

<p>수식으로 표현 된 Neural Network을 보면, bias vector인 $b^{(1)}$, $b^{(2)}$, Weight matrix인 $W^{(1)}$, $W^{(2)}$, 그리고 activation function인 $G$와 $s$로 구성되어 있음을 확인할 수 있다. 여기서 $ h(x) = s(b^{(1)} + W^{(1)} x) $는 hidden layer를 표현하며, output layer는 $ o(x) = G(b^{(2)} + W^{(2)} x) $로 표현된다. Activation Function $s$는 보통 $tanh$, $sigmoid$, $ReLU$ 등의 함수들이 사용된다.</p>

<div class="img_row">
  <img class="col two display: inline-block;" src="/images/201604/700px-Gjl-t(x).svg.png" />
  <img class="col one" src="/images/201604/activation_funcs1.png" />
</div>
<div class="col three caption">
  여러 Activation 함수들의 특징. logical classification에 적절한 동작을 할 수 있도록 해준다.
</div>

<p>Theano 예제에서는 $tanh$를 사용하겠다고 하였으며, 이는 학습속도가 빠르기 때문이라고 한다. coursera 강의에서는 $sigmoid$ 함수를 이용하며, udacity의 tensorflow강의에서는 $ReLU$ 함수를 이용한다. 개발하면서 적절한 것을 선택해서 사용하면 되는 문제로 보인다.</p>

<p>output vector는 $o(x) = G(b^{(2)} + W^{(2)} h(x))$의 연산을 통해 구할 수 있다. 이 연산은 앞서 다룬 MNIST Logistic Regression에서 사용하는 연산과 동일한 연산이며, activation 함수 $G$로는 $softmax$를 이용한다.</p>

<h1 id="hidden-layer-class-구성">Hidden Layer Class 구성</h1>

<p>MLP를 구성하는데 사용할 Hidden Layer를 구성하는데 코드를 재사용하기 위하여 Hidden Layer class를 구성한다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">HiddenLayer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">activation</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">tanh</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span>
    
    <span class="c"># Weight matrix 초기화</span>
    <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">W_values</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
          <span class="n">low</span><span class="o">=-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">)),</span>
          <span class="n">high</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">)),</span>
          <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
      <span class="p">)</span>
      <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="n">theano</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">:</span>
          <span class="n">W_values</span> <span class="o">*=</span> <span class="mi">4</span>

      <span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">W_values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'W'</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c"># bias vector 초기화</span>
    <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">b_values</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_out</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">b_values</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>

    <span class="n">lin_output</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">lin_output</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="bp">None</span>
      <span class="k">else</span> <span class="n">activation</span><span class="p">(</span><span class="n">lin_output</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c"># parameters of the model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span></code></pre></figure>

<p>여기서 Weight Matrix에 random number를 임의로 주지 않고 uniformly distribution한 random number로서, 범위를 정해서 주는 이유는 activation function의 유효한 범위 때문이라고 한다.</p>

<p>각각의 activation 함수에 따른 유효 범위는 다음과 같다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">$tanh$</th>
      <th style="text-align: center">$sigmoid$</th>
      <th style="text-align: center">$LeLU$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">low</td>
      <td style="text-align: center">$- \sqrt {\frac {6}{n_{in} + n_{out}}}$</td>
      <td style="text-align: center">$-4\sqrt {\frac {6}{n_{in} + n_{out}}}$</td>
      <td style="text-align: center">(?)</td>
    </tr>
    <tr>
      <td style="text-align: center">high</td>
      <td style="text-align: center">$\sqrt {\frac {6}{n_{in} + n_{out}}}$</td>
      <td style="text-align: center">$4\sqrt {\frac {6}{n_{in} + n_{out}}}$</td>
      <td style="text-align: center">(?)</td>
    </tr>
  </tbody>
</table>

<h1 id="mlp-class-구성">MLP Class 구성</h1>

<p>이제 Hidden layer와 ouput layer를 묶어서 하나의 MLP로 구성할 것이다.
output layer는 MNIST logistic regression과 동일한 구성을 갖는다. 이 예제는 하나의 hidden layer만을 사용하므로, MLP의 paramter 중 hidden layer의 unit의 수를 지정하는 <em>n_hidden</em>에 대해서도 하나만 존재한다. 이 부분에 대해서는, hidden layer가 여러개가 되었을 때엔 array로 지정할 수 있도록 변형 될 것이다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="s">"""Multi-Layer Perceptron Class"""</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
    <span class="s">"""Initialize the parameters for the multilayer perceptron"""</span>

    <span class="c"># Hidden Layer 정의</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hiddenLayer</span> <span class="o">=</span> <span class="n">HiddenLayer</span><span class="p">(</span>
        <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
        <span class="n">n_in</span><span class="o">=</span><span class="n">n_in</span><span class="p">,</span>
        <span class="n">n_out</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">tanh</span>
    <span class="p">)</span>

    <span class="c"># Output Layer에 정의. MNIST 예제에서 사용한 Logistic Regression을 적용한다.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenLayer</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
        <span class="n">n_in</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
        <span class="n">n_out</span><span class="o">=</span><span class="n">n_out</span>
    <span class="p">)</span>
    
    <span class="c"># Regularization Opetions</span>
    <span class="c"># L1 norm</span>
    <span class="c"># one regularization option is to enforce L1 norm to be small</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">L1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenLayer</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c"># square of L2 norm ; </span>
    <span class="c"># one regularization option is to enforce square of L2 norm to be small</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">L2_sqr</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenLayer</span><span class="o">.</span><span class="n">W</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span><span class="o">.</span><span class="n">W</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c"># Output layer의 cost 함수를 MLP의 cost 함수로 지정한다.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">negative_log_likelihood</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span><span class="o">.</span><span class="n">negative_log_likelihood</span>
    <span class="p">)</span>
    <span class="c"># 예측한 값의 오류를 확인하고자 하는 errors의 함수도 MLP와 output layer에 동일하게 적용한다.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span><span class="o">.</span><span class="n">errors</span>

    <span class="c"># the parameters of the model are the parameters of the two layer it is</span>
    <span class="c"># made out of</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenLayer</span><span class="o">.</span><span class="n">params</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">logRegressionLayer</span><span class="o">.</span><span class="n">params</span>
    <span class="c"># end-snippet-3</span>

    <span class="c"># keep track of model input</span>
    <span class="bp">self</span><span class="o">.</span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span></code></pre></figure>

<h1 id="processing">Processing</h1>

<p>이제 Symbolic Expresion으로 명세한 MLP를 기반으로 Deep Learning을 해보자.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_mlp</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">L1_reg</span><span class="o">=</span><span class="mf">0.00</span><span class="p">,</span> <span class="n">L2_reg</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
             <span class="n">dataset</span><span class="o">=</span><span class="s">'mnist.pkl.gz'</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
  <span class="s">"""
  Demonstrate stochastic gradient descent optimization for a multilayer
  perceptron

  This is demonstrated on MNIST.

  :type learning_rate: float
  :param learning_rate: learning rate used (factor for the stochastic
  gradient

  :type L1_reg: float
  :param L1_reg: L1-norm's weight when added to the cost (see
  regularization)

  :type L2_reg: float
  :param L2_reg: L2-norm's weight when added to the cost (see
  regularization)

  :type n_epochs: int
  :param n_epochs: maximal number of epochs to run the optimizer

  :type dataset: string
  :param dataset: the path of the MNIST dataset file from
               http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz


 """</span>
  <span class="c"># Data Load</span>
  <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

  <span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

  <span class="c"># 사용할 데이터의 batch의 개수 계산</span>
  <span class="n">n_train_batches</span> <span class="o">=</span> <span class="n">train_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
  <span class="n">n_valid_batches</span> <span class="o">=</span> <span class="n">valid_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
  <span class="n">n_test_batches</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>

  <span class="c">######################</span>
  <span class="c"># BUILD ACTUAL MODEL #</span>
  <span class="c">######################</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'... building the model'</span><span class="p">)</span>

  <span class="c"># allocate symbolic variables for the data</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span>  <span class="c"># index to a [mini]batch</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>  <span class="c"># the data is presented as rasterized images</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>  <span class="c"># the labels are presented as 1D vector of</span>
                      <span class="c"># [int] labels</span>

  <span class="n">rng</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>

  <span class="c"># construct the MLP class</span>
  <span class="n">classifier</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
      <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
      <span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
      <span class="n">n_in</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span>
      <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
      <span class="n">n_out</span><span class="o">=</span><span class="mi">10</span>
  <span class="p">)</span>

  <span class="c"># cost function;</span>
  <span class="c"># linear regression에 대한 것도 고려한 cost 함수를 사용한다.</span>
  <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">classifier</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
      <span class="o">+</span> <span class="n">L1_reg</span> <span class="o">*</span> <span class="n">classifier</span><span class="o">.</span><span class="n">L1</span>
      <span class="o">+</span> <span class="n">L2_reg</span> <span class="o">*</span> <span class="n">classifier</span><span class="o">.</span><span class="n">L2_sqr</span>
  <span class="p">)</span>
  <span class="c"># end-snippet-4</span>

  <span class="c"># test, validation 함수 정의. MINST 예제와 동일하다.</span>
  <span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>

  <span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>

  <span class="c"># MLP의 각 layer마다 gradient를 계산한다.</span>
  <span class="n">gparams</span> <span class="o">=</span> <span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>

  <span class="c"># gradient연산은 update와 함께 연동되어 learning parameter를 gradient 계산과 함께 더한다.</span>
  <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span>
      <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gparam</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">gparam</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">gparams</span><span class="p">)</span>
  <span class="p">]</span>

  <span class="c"># train model 함수 역시 MINST 예제와 동일하게 명세한 update 함수를 적용하여 사용한다.</span>
  <span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
      <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>

  <span class="c">###############</span>
  <span class="c"># TRAIN MODEL #</span>
  <span class="c">###############</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'... training'</span><span class="p">)</span>

  <span class="c"># early-stopping parameters</span>
  <span class="n">patience</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c"># look as this many examples regardless</span>
  <span class="n">patience_increase</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c"># wait this much longer when a new best is</span>
                         <span class="c"># found</span>
  <span class="n">improvement_threshold</span> <span class="o">=</span> <span class="mf">0.995</span>  <span class="c"># a relative improvement of this much is</span>
                                 <span class="c"># considered significant</span>
  <span class="n">validation_frequency</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">,</span> <span class="n">patience</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
                                <span class="c"># go through this many</span>
                                <span class="c"># minibatche before checking the network</span>
                                <span class="c"># on the validation set; in this case we</span>
                                <span class="c"># check every epoch</span>

  <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
  <span class="n">best_iter</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">test_score</span> <span class="o">=</span> <span class="mf">0.</span>
  <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>

  <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">done_looping</span> <span class="o">=</span> <span class="bp">False</span>

  <span class="k">while</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">n_epochs</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done_looping</span><span class="p">):</span>
      <span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="k">for</span> <span class="n">minibatch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">):</span>

          <span class="n">minibatch_avg_cost</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">minibatch_index</span><span class="p">)</span>
          <span class="c"># iteration number</span>
          <span class="nb">iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_train_batches</span> <span class="o">+</span> <span class="n">minibatch_index</span>

          <span class="k">if</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">validation_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
              <span class="c"># compute zero-one loss on validation set</span>
              <span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">validate_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span>
                                   <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_valid_batches</span><span class="p">)]</span>
              <span class="n">this_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">validation_losses</span><span class="p">)</span>

              <span class="k">print</span><span class="p">(</span>
                  <span class="s">'epoch </span><span class="si">%</span><span class="s">i, minibatch </span><span class="si">%</span><span class="s">i/</span><span class="si">%</span><span class="s">i, validation error </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span> <span class="o">%</span>
                  <span class="p">(</span>
                      <span class="n">epoch</span><span class="p">,</span>
                      <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                      <span class="n">n_train_batches</span><span class="p">,</span>
                      <span class="n">this_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span>
                  <span class="p">)</span>
              <span class="p">)</span>

              <span class="c"># if we got the best validation score until now</span>
              <span class="k">if</span> <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span><span class="p">:</span>
                  <span class="c">#improve patience if loss improvement is good enough</span>
                  <span class="k">if</span> <span class="p">(</span>
                      <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span> <span class="o">*</span>
                      <span class="n">improvement_threshold</span>
                  <span class="p">):</span>
                      <span class="n">patience</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">*</span> <span class="n">patience_increase</span><span class="p">)</span>

                  <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">this_validation_loss</span>
                  <span class="n">best_iter</span> <span class="o">=</span> <span class="nb">iter</span>

                  <span class="c"># test it on the test set</span>
                  <span class="n">test_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span>
                                 <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_batches</span><span class="p">)]</span>
                  <span class="n">test_score</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_losses</span><span class="p">)</span>

                  <span class="k">print</span><span class="p">((</span><span class="s">'     epoch </span><span class="si">%</span><span class="s">i, minibatch </span><span class="si">%</span><span class="s">i/</span><span class="si">%</span><span class="s">i, test error of '</span>
                         <span class="s">'best model </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span><span class="p">)</span> <span class="o">%</span>
                        <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_train_batches</span><span class="p">,</span>
                         <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">))</span>

          <span class="k">if</span> <span class="n">patience</span> <span class="o">&lt;=</span> <span class="nb">iter</span><span class="p">:</span>
              <span class="n">done_looping</span> <span class="o">=</span> <span class="bp">True</span>
              <span class="k">break</span>

  <span class="n">end_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
  <span class="k">print</span><span class="p">((</span><span class="s">'Optimization complete. Best validation score of </span><span class="si">%</span><span class="s">f </span><span class="si">%% </span><span class="s">'</span>
         <span class="s">'obtained at iteration </span><span class="si">%</span><span class="s">i, with test performance </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span><span class="p">)</span> <span class="o">%</span>
        <span class="p">(</span><span class="n">best_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">,</span> <span class="n">best_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">))</span>
  <span class="k">print</span><span class="p">((</span><span class="s">'The code for file '</span> <span class="o">+</span>
         <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">__file__</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
         <span class="s">' ran for </span><span class="si">%.2</span><span class="s">fm'</span> <span class="o">%</span> <span class="p">((</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="mf">60.</span><span class="p">)),</span> <span class="nb">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span></code></pre></figure>

  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://localhost:4000/metal/2016-04-25-theano-multilayer-perceptron/%20via%20&#64;&hashtags=DeepLearning,Neural Networks,Theano,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/metal/2016-04-25-theano-multilayer-perceptron/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://localhost:4000/metal/2016-04-25-theano-multilayer-perceptron/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/metal/2016-04-25-theano-multilayer-perceptron/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
