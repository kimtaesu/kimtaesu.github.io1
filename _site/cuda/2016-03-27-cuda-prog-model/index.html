<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>CUDA 프로그래밍 모델</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/cuda/2016-03-27-cuda-prog-model/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*kimtaesu</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- kimtaesu instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/android">Android</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/kotlin">Kotlin</a>
          
        
          
        
          
          <a class="page-link" href="/python">Python</a>
          
        
          
          <a class="page-link" href="/self-improvement/">Self-Improve</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">CUDA 프로그래밍 모델</h1>
    <p class="post-meta">March 27, 2016</p>
  </header>

  <article class="post-content">
    <p>CUDA 프로그래밍을 하면서 제가 겪었던, 또는 주위에 CUDA 프로그래밍을 주변에 소개하면서 겪었던 가장 큰 어려움은 CUDA 프로그래밍의 개념을 이해하거나 이해시키는 일이었습니다.</p>

<p>일반적인 병렬처리 기법으로 많이 사용하는 멀티쓰레드나 멀티프로세싱, 그리고 그것들의 온전한 동작을 위해 운영체제에서 제공하는 다양한 도구들은 그나마 개념적으로나마 이해할 수 있었지만, CUDA는 아무래도 다른 하드웨어에서 다른 동작 조건을 갖고 동작하기 때문인 것 같습니다. 서너개의 병렬처리를 상상하기도 벅찬데 수백개의 멀티쓰레딩을 할 수 있다는 것은 장점으로 느껴지다가도 다시 한번 생각하면, 그걸 어떻게 하는 건지 겁이 나는 것도 사실입니다.</p>

<p>때문에 CUDA Programming Model은 CUDA 프로그래밍을 하는데 있어서 중요한 개념을 제공해 줍니다.</p>

<ul>
  <li><a href="#SIMT">CUDA Processing 모델 (SIMT Architecture)</a></li>
  <li><a href="#Kernels">CUDA Kernels</a></li>
  <li><a href="#Thread Hierachy">CUDA Thread Hierachy</a></li>
  <li><a href="#Memory">Memory 계층</a></li>
  <li><a href="#HC">Heterogeneous Computing</a></li>
  <li><a href="#Compute Capability">Compute Capability</a></li>
</ul>

<p>본격적으로 CUDA 프로그래밍을 시작하기에 앞서서 주요 개념들을 살펴보도록 하겠습니다.</p>

<p><br /></p>

<p><a id="SIMT" class="anckor"></a></p>

<h1 id="simt-single-instruction-multiple-thread">SIMT (Single Instruction Multiple Thread)</h1>
<p>SIMT라는 개념은 nVidia에서 자신의 CUDA 동작을 설명하기 위해서, CPU에서 사요하는 용어를 차용해서 만든 조어입니다. CPU에서는 주로 SIMD(Single Instruction Multiple Data)라는 용어를 사용하는데, CPU의 성능을 최대한 활용하기 위해서 하나의 명령어로 여러개의 데이터를 처리하도록 하는 동작을 의미합니다. 이와 비슷하게 CUDA는 하나의 명령어로 여러개의 쓰레드를 동작시킵니다. 그러면 각각의 CUDA 쓰레드는 데이터를 하나씩 처리하지만, CUDA에서 여러개의 쓰레드를 동작시키므로 동시에 여러개의 데이터를 처리할 수가 있습니다.</p>

<p>즉, 용어와 방법은 다르지만 SIMD와 SIMT는 동일한 목적인 데이터를 병렬로 처리한다는 개념을 갖고 있습니다. 이는 앞으로 CUDA 프로그래밍을 하는데 있어서 중요하면서도 기본적인 개념이므로 새로운 개념을 접하시거나 코딩하실때 염두에 두고 개발하셔야 합니다.</p>

<p><a id="Kernels" class="anckor"></a></p>

<h1 id="kernels">Kernels</h1>
<p>Kernel은 GPU에서 처리하는 동작 내지는 함수를 의미하며, GPU가 처리하는 function 코드라고 볼 수 있습니다. 즉, 앞서 소개한 SIMT 개념을 담고 작성된 CUDA 코드입니다.</p>

<p>예제를 통해서 살펴보도록 하겠습니다.
C를 기반으로 하므로, C 언어를 기준으로 보셔야 합니다.</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13</pre></td><td class="code"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">VecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="p">...</span>
    <span class="c1">// Kernel invocation with N threads
</span>    <span class="n">VecAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
<span class="p">...</span>
<span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<p>위 코드에서 <code class="highlighter-rouge">__global__</code> 키워드가 달려있는 함수인 <code class="highlighter-rouge">VecAdd</code>를 kernel이라고 부릅니다.
한편, line 11의 <code class="highlighter-rouge">Vector&lt;&lt;&lt;M, N&gt;&gt;&gt;(...)</code>는 커널을 호출하는 라인입니다. 각 부분이 구체적으로 어떤 것을 의미하는지는 다음 절에서 설명드리도록 하겠습니다.</p>

<p>이와 비슷한 CUDA Keyword로는 <code class="highlighter-rouge">__device__</code>, <code class="highlighter-rouge">__host__</code> 등이 있습니다. 각각이 의미하는 바는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>키워드</th>
      <th>의미</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>global</strong></td>
      <td>CPU Main thread에서 호출 가능한 GPU에서 동작하는 함수</td>
    </tr>
    <tr>
      <td><strong>host</strong></td>
      <td>CPU Thread에서만 동작하는 함수</td>
    </tr>
    <tr>
      <td><strong>device</strong></td>
      <td>GPU 커널함수 내부에서만 호출 가능한 GPU 함수</td>
    </tr>
    <tr>
      <td><strong>host</strong> <strong>device</strong></td>
      <td>CPU/GPU 함수 각각 모두 호출 가능한 함수</td>
    </tr>
  </tbody>
</table>

<p>이러한 CUDA 함수 키워드들은 CUDA 컴파일러로 하여금 코드의 동작 대상을 알려주어 개발자가 원하는 동작을 할 수 있도록 도와주는 한편, 개발하는 분들에게도 함수가 어떤 동작조건으로 동작하는지 알려주는 역할을 한다고 할 수 있겠습니다.</p>

<p>중요한 점은, CUDA 프로그래밍은 커널이라는 개념을 GPU에서 동작하는 함수라고 부르며, 이를 기준으로 CPU와 GPU 동작을 구분한다는 것입니다. 그래서 같은 파일에 있는 소스코드이지만 키워드를 기반으로 동작하는 하드웨어는 다르도록 개발 할 수 있습니다.</p>

<p><a class="anckor" id="Thread Hierachy"></a></p>

<h1 id="thread-hierachy">Thread Hierachy</h1>
<p>앞서 설명드린대로 CUDA는 SIMT 구조를 갖고 있으며, 여러개의 Thread를 동작시킵니다. 다만 스스로 알아서 하는 것은 아니고 얼마나 많은 쓰레드를 동작시켜야만 하는지 알려주어야만 합니다.</p>

<p>이때 알아야 하는 것이 Thread Hierachy입니다.</p>

<p><img src="/images/201603/CUDA grid and block.png" width="400px" /></p>

<p>그림을 보시면 CUDA Thread는 CUDA Block 또는 CUDA Thread Block으로 묶여서 동작합니다. 그리고 CUDA Block은 다시 Grid라는 단위로 묶여서 동작합니다.</p>

<p>앞에서 사용한 예제 코드를 다시 확인해 보도록 하겠습니다.</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17</pre></td><td class="code"><pre><span class="c1">// Kernel definition
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatAdd</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span>
                       <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="p">...</span>
    <span class="c1">// Kernel invocation with one block of N * N * 1 threads
</span>    <span class="kt">int</span> <span class="n">numBlocks</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">dim3</span> <span class="n">threadsPerBlock</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
    <span class="n">MatAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
    <span class="p">...</span>
<span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<p><code class="highlighter-rouge">line 5,6</code>는 CUDA 쓰레드의 인덱스를 변수로 지정하는 라인이며, <code class="highlighter-rouge">line 7</code>은 앞에서 알게된 쓰레드 인덱스로 연산을 수행하는 부분입니다. 여기서 CUDA 쓰레드는 앞서 설명드린 SIMT 구조에서 말하는 쓰레드이며, CUDA에서는 각각의 쓰레드를 구별할 수 있도록 <code class="highlighter-rouge">threadIdx</code> 구조체를 제공합니다. 그래서 CUDA 쓰레드는 자신의 index를 동작 중에 알 수 있게 되고, 어떤 데이터를 처리할 것인지 인덱스 정보를 바탕으로 판단하게 됩니다.</p>

<p>그렇다면 몇 개를 동작시킬 것인지는 CUDA 커널은 어떻게 알 수 있을까요?
<code class="highlighter-rouge">line 80</code>에 있는 <code class="highlighter-rouge">&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;</code>은 그 답을 알고 있습니다.
위 코드를 직역하자면 <code class="highlighter-rouge">블럭 하나에, thread는 NxN개를 실행시켜라</code>라는 것인데, <code class="highlighter-rouge">&lt;&lt;&lt; ... &gt;&gt;&gt;</code>을 통해 CUDA 커널의 제어 파라미터를 전달해 주는 것입니다.</p>

<p><code class="highlighter-rouge">&lt;&lt;&lt; ... &gt;&gt;&gt;</code>에는 CUDA block의 개수와 Block Size(CUDA Thread의 개수)를 입력해줍니다. 위 예제의 경우에는 block 개수는 1, thread의 수는 NxN으로 정해놓은 것입니다. 그렇다면 NxN개는 말 그대로 아무크기나 지정해도 되는 것일까요?</p>

<p>실제로는 그렇지 않으며, CUDA 하드웨어 스펙에 의해 최대 크기가 정해져 있고, 최적 사이즈도 대체적으로 정해져있는 편입니다. 제 경험상 <code class="highlighter-rouge">NxN = 256</code>인 경우가 보통은 최적 크기입니다. 즉 <code class="highlighter-rouge">(16)x(16)</code>이라는 이야기인데, 이것은 그리 크지 않은 크기입니다. 그렇다면 그 이상의 데이터를 처리해야하는 경우에는 어떻게 해야할까요? 쉽게 생각하면, 우리가 만든 CUDA block을 다른 데이터를 지정해서 여러번 실행시키면 될 것입니다. CUDA에서는 이것을 Grid라는 개념으로 동작을 하며, <code class="highlighter-rouge">gridDim</code>을 통해서 지정할 수 있습니다. 즉, 몇 번 block이 재사용되어야 하는지 CUDA 커널에 알려주고, CUDA는 위 그림에서 Grid와 같은 형태로 Block을 전개하고, block 내부에서는 여러개의 Thread를 동작시키는 것입니다.</p>

<p>이를 나타낸 예제를 갖고 설명을 드리겠습니다. 이 예제는 매우 큰 <code class="highlighter-rouge">NxN</code>개의 데이터를 CUDA에서 처리하는 동작입니다.</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18</pre></td><td class="code"><pre><span class="c1">// Kernel definition
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatAdd</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span>
<span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="p">...</span>
    <span class="c1">// Kernel invocation
</span>    <span class="n">dim3</span> <span class="n">threadsPerBlock</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">);</span>
    <span class="n">dim3</span> <span class="n">numBlocks</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">N</span> <span class="o">/</span> <span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>
    <span class="n">MatAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
    <span class="p">...</span>
<span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></figure>

<p><code class="highlighter-rouge">line 14</code>를 보시면 제가 조금전에 설명드린대로 일반적인 최적 사이즈라고 부르는 대로 <code class="highlighter-rouge">[16, 16]</code>크기의 thread block을 구성했습니다. 그리고 numBlocks의 크기는 N을 thread block의 크기로 나누어 CUDA block이 몇 번 실행되어야 하는지 계산을 했습니다. (정확한 계산은 좀 다르지만 이 부분은 차후에 더 설명을 드리겠습니다.) 이떄 x와 y에 대해서 각각 계산을 해주어서 2차원 인덱스를 구성했다는 점입니다.</p>

<p>이어서 CUDA Kernel을 살펴보도록 하겠습니다. <code class="highlighter-rouge">line 5,6</code>을 보시면 처음보는 키워드 들이 들어가 있습니다. <code class="highlighter-rouge">blockIdx</code>, <code class="highlighter-rouge">blockDim</code>은 CUDA Thread block의 인덱스와 크기를 나타내는 키워드입니다. 특히 <code class="highlighter-rouge">blockDim</code>은 앞서 CUDA Kernel를 호출할때 <code class="highlighter-rouge">line 16</code>에서 <code class="highlighter-rouge">&lt;&lt;&lt; ... &gt;&gt;&gt;</code>에 전달해준 제어 파라미터 중 threadPerBlock와 동일한 값을 갖습니다. 즉, 어떻게 <code class="highlighter-rouge">NxN</code> 크기의 데이터를 쪼개서 처리할 것인지 CUDA로 하여금 알 수 있게 해주는 키워드입니다. 이것에 인덱스 값을 추가로 계산하면 CUDA Thread의 Index를 알 수 있게 되고, 처리할 데이터의 인덱스를 알 수 있게 됩니다.</p>

<p>이렇게 복잡하고 다양한 인덱싱을 지원하기 위해서 CUDA는 총 5가지 키워드를 제공하고 있습니다.</p>

<table>
  <thead>
    <tr>
      <th>키워드</th>
      <th>설명</th>
      <th>차원</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gridDim</td>
      <td>Kernel의 block의 수</td>
      <td>x, y</td>
    </tr>
    <tr>
      <td>blockIdx</td>
      <td>CUAD block의 인덱스</td>
      <td>x, y</td>
    </tr>
    <tr>
      <td>blockDim</td>
      <td>CUDA block의 크기</td>
      <td>x, y, z</td>
    </tr>
    <tr>
      <td>threadIdx</td>
      <td>CUDA 쓰레드의 인덱스</td>
      <td>x, y, z</td>
    </tr>
    <tr>
      <td>warpSize</td>
      <td>CUDA Instruction이 동시에 처리가능한 CUDA 쓰레드 수</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>즉, 크기를 알려줘야하는 Grid와 Block에는 <code class="highlighter-rouge">-Dim</code>이라는 이름의 키워드가, index를 알아야만 하는 block과 thread에는 <code class="highlighter-rouge">-Idx</code>라는 이름의 키워드가 있는 것입니다.</p>

<p>이를 잘 조합한다면 CUDA Kernel 코드는 인덱싱만 잘해도 하나의 쓰레드만 처리하는 코드를 짜도 병렬처리가 가능하게 됩니다. 앞서 설명 드렸던 SIMT 구조가 이렇게 드러나는 것이죠.</p>

<p>인덱싱에 대한 대표적인 예를 나열해 보겠습니다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">인덱싱 종류</th>
      <th style="text-align: left">코드</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1차원 데이터(x only)</td>
      <td style="text-align: left">blockDim.x * blockIdx.x + threadIdx.x</td>
    </tr>
    <tr>
      <td style="text-align: center">2차원 데이터(x, y)</td>
      <td style="text-align: left">blockDim.x * blockIdx.x + threadIdx.x, blockDim.y * blockIdx.y + threadIdx.y</td>
    </tr>
    <tr>
      <td style="text-align: center">2차원 데이터(x only)</td>
      <td style="text-align: left">N.x * blockDim.y * blockIdx.y + N.x * threadIdx.y + blockDim.x * blockIdx.x + blockIdx.x</td>
    </tr>
  </tbody>
</table>

<p>위 표에서 2번째와 3번째는 2차원 인덱스를 한다는 점에서는 동일하지만 위의 것은 2차원 배열을 사용해야만 하고, 3번째는 1차원 배열에 저장된 2차원 데이터에 대하여 2차원 인덱싱을 해준다는 점에서 차이가 있습니다. 세번째가 복잡해서 다음과 같이 변경해서 사용할 수도 있습니다.</p>

<blockquote>
  <p>idx_x = blockDim.x * blockIdx.x + threadIdx.x
idx_y = blockDim.y * blockIdx.y + threadIdx.y
idx = idx_y * N.x + idx_x</p>
</blockquote>

<p>따지고보면 표의 세번째와 같은 연산임을 알 수 있는데, 정확히 뭘 하려는 것인지 코드상으로 알 수 있고 실수해도 바로 알 수 있다는 장점이 있습니다.</p>

<p>끝으로, Warp이라는 용어가 위 표에서 나왔었는데요, 표에 있는대로 Warp은 CUDA에서 동시에 처리가능한 또는 동시에 처리하는 CUDA Thread 수 입니다. 병렬처리 연산을 수행하는데 Warp을 단위로 작업을 Scheduling하고 Warp 개수 만큼을 병렬처리 할 수 있습니다. Warp의 크기는 CUDA Compute Capacity에 따라서 다를 수 있으며 보통은 32개 입니다. 이 크기는 가변적으로 설정할 수는 없습니다. 다만 코드의 내용에 따라 Warp을 충분히 활용하지 못해서 CUDA Thread가 대기상태로 놓일 수는 있으며 이를 따지는 것이 CUDA Occupancy입니자만, 이부분은 별도의 포스트에서 다루도록 하겠습니다.</p>

<p>제가 드린 설명이 이해가 되시나요? ^^;
CUDA 프로그래밍을 하시는데는 이정도면 충분하지만, 정확한 이해를 위해서는 CUDA Processor에 대한 설명도 필요한데요, 이부분에 대해서는 별도의 포스팅으로 설명을 드리도록 하겠습니다.</p>

<p><a class="anckor" id="Memory"></a></p>

<h1 id="memory-계층">Memory 계층</h1>

<p>CUDA에서 사용하는 메모리 계층은 크게 두 가지로 구별 할 수 있습니다.</p>

<ol>
  <li>Global Memory
    <ul>
      <li>그래픽 카드에 설치되는 메모리. 1G이상. 다른 메모리에 비해 느리다. 물론 CPU에서 사용하는 메모리보다는 빠르다. 대역폭에 신경을 써야 한다.</li>
    </ul>
  </li>
  <li>On-Chip memory
    <ul>
      <li>Shared L2 Cache: CUDA Core간 공유하는 Cache 메모리. 프로그래밍에 노출되지 않음.</li>
      <li>Shared L1 Cache: CUDA Core 내에서 사용하는 L1 Cache. (약 64KB. 코어 버전마다 다름). 프로그래밍 가능</li>
      <li>Texture Memory: CUDA Core가 공유하는 Pre-fetch를 지원하는 read-only cache. 프로그래밍 가능.</li>
      <li>Constant Memory: CUDA Core가 공유하는 Kernel 실행중 업데이트가 불가능한 read-only cache. 프로그래밍 가능.</li>
      <li>Register File: CUDA Thread가 사용하는 Register</li>
    </ul>
  </li>
</ol>

<p>각각의 메모리는 개별적인 설명이 필요한데, 이 부분을 잘 이해하는 것이 CUDA 프로그래밍을 최적화 시켜나가는데 중요한 부분이라고 할 수 있습니다. 아래 그림은 CUDA Core의 그림입니다. 위에서 설명 드린 것들이 곳곳에 있습니다만, 보이시나요? ^^;;</p>

<p>아래 정사각형 그림이 CUDA Multiprocessor라고 불리는 것이고, 위는 그것이 여러개가 모여있는, 우리가 흔히들 GPU 코어라고 부르는 것의 내부 모습입니다.</p>

<p><img src="/images/201603/blockdiagram_big.png" width="600" /></p>

<p><img src="/images/201603/Kepler.png" width="300" /></p>

<p>이 그림과 CUDA Thread간의 상호작용을 하는 것이 CUDA 프로그래밍인데요, CUDA 하드웨어의 구조와 CUDA Thread간의 관계, 그리고 메모리가 연동되는 부분은 별도의 포스팅으로 설명을 드리도록 하겠습니다.</p>

<p><a class="anckor" id="HC"></a></p>

<h1 id="heterogenious-computing">Heterogenious Computing</h1>

<p>Heterogeneious Computing은 예전에 하나의 코어에서 연산을 모두 처리하던 것에서 별도의 연산을 위한 하드웨어를 통해서 컴퓨터의 처리성능을 높이는 컴퓨팅 방식을 말합니다. DSP, FPGA 등을 활용한 것들이 예를 들 수 있겠는데요, CUDA 또한 CPU의 제어 하에서 고성능의 연산을 CPU 대신 처리해 준다는 점에서 비슷하다고 할 수가 있겠습니다.</p>

<p>CUDA에서는 CPU에서 사용하는 Host 메모리와 그래픽 카드에 있는 Global (Device) Memory간의 데이터 전송 및 Kernel 호출 등, CUDA의 올바른 동작을 위해 필요한 절차 등을 의미합니다.</p>

<p><a class="anckor" id="Compute Capability"></a></p>

<h1 id="compute-capability">Compute Capability</h1>

<p>Compute Capability는 CUDA 코어의 하드웨어 버전을 의미하며, CUDA 개발을 위해 여러분이 설치하셨을 CUDA 버전과는 별개의 것입니다. 버전에 따라서 캐시의 크기와 코어의 개수 등 여러 부분에 대해서 성능과 기능이 개선되어 왔으며, 이에 따라 최적화 결과가 조금씩 달라질 수 있습니다.</p>

<p>이것에 관해서 한가지 말씀드릴 수 있는 것은, 성능과는 별개로 버전이 높을수록 최적화 문제 등에 여러분이 개발에서 겪을 어려움을 많이 줄일 수 있을 것이란 것입니다.</p>

<p>CUDA는 현재 버전이 7.5이며, Compute Capability 버전은 5.3입니다. 호환성 문제를 생각해볼떄, 여러분의 SW를 배포할 경우 CUDA 버전은 고려하지 않으셔도 되지만, Compute Capability는 고려하셔야 합니다. 만약 여러분이 최신의 Compute Capability만을 지원한다면 여러분의 SW를 배포받은 분들은 지원이 되지않을 확률이 아무래도 높겠죠. 왜냐하면 여러분은 최신의 그래픽카드를 사용하지만, 대부분의 사람들은 한번 사놓고 몇년간 하드웨어를 교체하지 않으니까요.</p>

<p>그보다도 더 문제는… nVidia가 CUDA Compute Capability 를 빠른 시일내에 올리고 있고, 제가 가지고 있는 GPU가 얼마 되지도 않아서 구형이 되어버린다는 것이겠죠. 예를 들어, Google의 Deep Learning 라이브러리인 Tensorflow는 CUDA Compute Capability의 최 하위 버전이 3.5 입니다. 비공식적으로 3.0부터 지원한다고는 하지만 개발자가 테스트를 하지 않았다는 것은 얼마든지 문제가 발생할 소지를 가지고 있는 것이니, 신경을 쓰셔야 합니다.</p>

<p>CUDA Compute Capability에 대한 상세한 스펙은 <a href="https://en.wikipedia.org/wiki/CUDA#Supported_GPUs">CUDA Wikipedia</a>를 참고하시기 바랍니다.</p>


  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://localhost:4000/cuda/2016-03-27-cuda-prog-model/%20via%20&#64;&hashtags=CUDA,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/cuda/2016-03-27-cuda-prog-model/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://localhost:4000/cuda/2016-03-27-cuda-prog-model/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/cuda/2016-03-27-cuda-prog-model/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
