<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>GPGPU와 CUDA</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="/cuda/2016-02-26-gpgpu-cuda/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*hanjack</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- hanjack instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/cuda/">CUDA</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/metal/">Machine Learning</a>
          
        
          
          <a class="page-link" href="/portfolio/">Gallery</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">GPGPU와 CUDA</h1>
    <p class="post-meta">February 26, 2016</p>
  </header>

  <article class="post-content">
    <h3 id="cpu-vs-gpu--gpgpu">CPU vs GPU, 그리고 GPGPU</h3>
<p>#### CPU와 GPU의 성능 비교
일반적으로 Computer에서 연산을 처리하는 모듈은 프로그램의 실행 및 시스템 자원의 관리 등에 특화 되어 있는 CPU와 영상처리를 위한 GPU로 구성되어 있습니다. 이중에서 GPU라는 것은 흔히 그래픽 카드라고 불리는 확장카드에 있는 코어를 말합니다. 과거에는 별도의 카드를 통해서 설치를 해야 했지만, 요즘은 칩 접적기술이 발달하게 되면서 대부분의 컴퓨터에서는 Mainboard에 함께 집적된 상태로 또는 CPU와 같은 칩셋에 집적된 상태로 판매되고 있습니다.</p>

<p><img src="/images//insidelogo.jpg" alt="상황이 이렇다보니 2014년 당시 그래픽 칩셋의 점유율 자체는 인텔이 가장 높습니다." /></p>

<p>하지만 이러한 그래픽 칩셋들은 어디까지나 일반적인 사용환경(오피스, 웹서핑, 영화 등)에 초점을 맞춘 그래픽 카드를 포함한 것이고, 게임을 원활이 즐기는데 충분한 그래픽카드라고는 할 수는 없습니다. 대신 확장 그래픽 카드, 그러니까 지금도 계속해서 PC에 확장카드 형태로 설치하는 그래픽 카드는 앞서 말한 온보드 내지는 온칩 형태의 그래픽 칩셋보다는 웬만해서는 좋은 그래픽스 연산 성능을 갖고 있습니다.</p>

<p>그렇다면 GPU의 연산 성능은 얼마나 CPU보다 좋을까요.</p>

<p><img src="/images//CPUvsGPU.png" alt="Intel CPU와 nVidia GPU간의 성능 비교" /></p>

<p>GPU의 성능은 2000년대 중반 이후부터는 CPU보다도 월등히 높은 성능을 보여주고 있는데, 이는 CPU가 구성되는 특성상 CPU가 가질수 밖에 없는 제약조건 때문입니다. 과거에는 Clock Speed를 높이고 집적도를 늘리면 성능이 증가했지만 인텔 혼자서는 더 이상의 성능을 내기 힘든 상태를 만나게 된 것입니다.</p>

<p><img src="/images//many_core_bw_sandia.jpg" alt="Sandia의  Multicore simulation 결과" /></p>

<p>위 자료는 Sandia에서 Multicore CPU를 시뮬레이션을 했을때 메모리가 시스템 성능에 미치는 영향을 보여줍니다. 즉, 인텔이 아무리 집적기술이 좋다고 하더라도 모두가 사용하는 PC환경에서는 CORE가 많은 것이 최선이 아니라는 것을 의미하는 것이죠.</p>

<p>하지만 GPU는 CPU와 다른 길을 갈 수 있습니다. 시스템의 자원들을 관리할 필요도 없다보니 데이터를 병렬적으로 처리하는 본연의 동작에 집중할 수 있고, 위 그래프에서와 같이 Core에 Memory를 올리는 것 처럼 GPU의 동작 조건을 구성할 수 있는 것이죠.</p>

<p>하지만 이렇게 성능을 높일 수 있다고 하더라도 제조사 입장에서는 고성능을 위해 비싸진 칩셋을 어떻게 팔아야할지 고민을 안할 수 없습니다. 또한 컴퓨터를 활용하여 실험이나 시뮬레이션을 해왔던 여러 연구원들의 입장에서는 CPU를 더 사고, Cluster를 구성해야만 하는 것에서 보다 저렴하면서도 좋은 성능을 낼 수 있는 수단이 필요했습니다.</p>

<h4 id="gpgpu-">GPGPU의 등장</h4>
<p>GPGPU (General Purpose Graphic Processing Unit)이란 개념은 점차 발전해가고 있는 GPU의 Computing 성능을 지금까지 사용해온 그래픽스 목적 외에 보다 범용적인 용도로 사용할 수 있도록 하자는 데서 출발합니다. CPU에서 SIMD(Single Instruction Multiple Data) 인스트럭션으로 연산 성능을 높이고 Assembly등을 통해서 연산 성능상의 제약을 극복하고자 했던 노력에서 GPU를 연산 가속기로 이용할 수 있게 된 것입니다.</p>

<p><img src="/images//220px-X86_extensions_2013.svg.png" alt="CPU에서 지원하는 SIMD 인스트럭션 목록" /></p>

<p>물론 그 당시에도 그래픽스 프로그래밍은 할 수 있었습니다. 하지만 OpenGL, DirectX를 공부하고, 쉐이더 언어(HLSL, GLSL, Cg 등)를 배워야 했으며, 그래픽스를 목적으로 만들어진 개발환경이라 아무래도 공부할 것도 많고 제약사항도 많았습니다. 이 글을 쓰고 있는 저 역시 저 언어들은 할 줄 모릅니다.</p>

<p>그 대신 GPGPU에 적합한 개발 환경이 등장하게 되면서, GPU프로그래밍을 그래픽스 분야 전문가 뿐만 아니라 C/C++을 할 수만 있다면 여러 연구분야에서 가져다 쓸 수 있도록 진입장벽이 낮아지게 됩니다. nVidia의 CUDA를 필두로 하여, <a href="https://www.khronos.org/opencl/">OpenCL</a>, DirectCompute 등이 그 주인공입니다.</p>

<p><img src="/images//NV_DesignedFor_CUDA_3D_sm.png" alt="" />
<img src="/images//OpenCL_Logo.png" alt="" />
<img src="/images//directx-11-logo.png" alt="" /></p>

<p>CUDA는 nVidia에서 개발한 언어라는 점에서 알 수 있듯이 nVidia 그래픽카드에서만 동작합니다. 반면 OpenCL는 OpenGL등을 발표하는 <a href="https://www.khronos.org">KronosGroup</a>에서 여러 회사에서 모인 표준안인지라 병렬지원을 하는 칩셋이라면 지원을 합니다. Apple에서 주도한 것으로 알려져 있고, 대표적으로 Intel, AMD의 CPU와 nVidia의 GPU와 CPU에서 동작합니다. DirectCompute는 DirextX11과 함께 출시가 되었습니다.</p>

<h4 id="section">무엇을 공부할 것인가</h4>
<p>그렇다면 어떤 것을 공부하면 좋을까요.
저는 개인적으로 CUDA를 공부하는 것을 추천합니다. nVidia 그래픽카드만 되는 것은 분명한 한계입니다. 하지만 Breakpoint가 가능한 디버거를 제공하고, 프로파일러, Best Practice Guide 등을 제공합니다. 따라서 쉽게 개발할 수 있고, 보유하고 있는 GPU자원을 최대한 끌어다 쓸 수가 있습니다.</p>

<p>하지만 다른 두가지 언어는 범용적이란 장점을 제외하면 개발이 수월하지 않습니다. 디버깅이 가능하지 않고, 여러 시스템을 지원해야하므로 최적화가 불가능합니다. 심지어는 제가 해보았을떄는 Best Practice로 개발한 Image Convolution CUDA 커널을 CUDA와 OpenCL로 옮기고 해보아도 OpenCL이 더 느렸습니다. 더욱이 Intel, AMD는 CPU에서만 지원합니다. 고성능을 목적으로 공부해서 사용하려는데 CPU에서 동작시킬 것이라면 저라면 차라리 OpenMP를 사용하겠습니다.</p>

<p>만약 부득이하게 OpenCL로 개발을 하셔야 한다면 CUDA로 개발을 한 이후에, OpenCL로 번역하실 것을 권합니다. nVidia가 CUDA개발을 해둔 상태에서 참여하게 되어서 많은 부분이 명칭만 다를 뿐 비슷한 개념을 가진 키워드들이 있습니다. 따라서 디버깅이 되지 않는 환경에서 굳이 애써 개발하시기 보다는 개발이 쉬운 CUDA에서 개발 한 이후에 진행하는 것이 쉽다는 것이 여러모로 유리합니다.</p>

<h4 id="gpu-">어떤 GPU를 살까</h4>
<p>nVidia의 그래픽카드의 제품군은 GeForce, Quadro, Tesla 등이 꾸준히 유지되고 있지만, 실제 그 안에서의 기술은 빠르게 변하고 있습니다. 전 이 도표를 2010년에 처음보고, nVidia가 과연 할 수 있을까 의심을 했었지만, 지금까지 nVidia는 자신의 약속을 지켜오고 있습니다.
<img src="/images//NVIDIA-2016-Roadmap-Pascal-GPU.jpg" alt="" /></p>

<p><a href="https://en.wikipedia.org/wiki/CUDA#Supported_GPUs">CUDA Capability</a>를 보면 CUDA Capability와 CUDA Core의 버전을 분류해놓은 테이블이 나옵니다. 기능은 버전이 올라갈 수록 아래 그림처럼 기능이 계속해서 추가 되어 왔습니다. 이런 부분은 CUDA 프로그래밍을 하면서 편리해지고 성능을 높이는데 많은 도움이 됩니다. 이 그림 말고도 참고해야할 것들이 많습니다만, 그건 본격적인 CUDA 프로그래밍을 할때 참고하는 것이 좋겠습니다.</p>

<p><img src="/images//tech_specs.jpg" alt="" /></p>

<p>중요한 것은, Goole에서 공개한 tensorflow는 CUDA capability 3.5버전을 최소 사양으로 정해놓았습니다. 제가 사용하는 GPU는 GeForce GTX 660이고 Core 세대는 Kepler인데, CUDA Capability는 3.0입니다. 다행인 것은 github에 임의로 3.0을 지원할 수 있도록 하는 <a href="https://github.com/tensorflow/tensorflow/issues/25">Tip</a>이 있습니다.</p>

<p>아무튼 새로 사실 예정이라면, CUDA Capability 3.5이상인 GPU를 구매하시는 것이 좋으며, 목록은 <a href="https://en.wikipedia.org/wiki/CUDA#Supported_GPUs">CUDA Capability</a>에서 예산에 맞는 가급적 가장 높은 Capability를 지원하는 카드를 사시는 것이 좋겠습니다.</p>

  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20/cuda/2016-02-26-gpgpu-cuda/%20via%20&#64;haanjack&hashtags=CUDA,GPGPU,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=/cuda/2016-02-26-gpgpu-cuda/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=/cuda/2016-02-26-gpgpu-cuda/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'haanjack';
        var disqus_title = '';
        var disqus_url = '/cuda/2016-02-26-gpgpu-cuda/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
