<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Denoising Autoencoder (dA) with Theano</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="/metal/2016-04-25-theano-denoising-autoencoder/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*hanjack</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- hanjack instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/cuda/">CUDA</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/metal/">Machine Learning</a>
          
        
          
          <a class="page-link" href="/portfolio/">Gallery</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Denoising Autoencoder (dA) with Theano</h1>
    <p class="post-meta">April 25, 2016</p>
  </header>

  <article class="post-content">
    <p>참조:</p>

<ol>
  <li><a href="http://deeplearning.net/tutorial/dA.html">Theano Denoising Autoencoder (dA)</a></li>
  <li><a href="">Wikipedia</a></li>
</ol>

<p>Denoising Autoencoder(dA)는 <strong>Autoencoder</strong>의 확장된 형태로 다음과 같이 구성되어 있다.</p>

<p><img class="col two center" src="/images/201604/Screen Shot 2016-04-25 at 2.52.01 PM.png" /></p>

<p>따라서, <strong>Autoencoder</strong>에 대해서 알아보고, Corruption을 추가했을때 어떻게 Denosing Autoencoder가 구현되는지 살펴 볼 것이다.</p>

<h1 id="autoencoder">Autoencoder</h1>

<p>Autoencoder는 unsupervised learning의 한종류로 입력 데이터의 representation을 학습키는 기법을 말한다. dimension reduction과 동일한 목적을 갖는다. 입력 unit의 수와 출력 unit의 수가 동일하며, 다른 어떤 데이터를 예측하도록 학습하는 것이 아니라 입력을 다시 복원할 수 있도록 학습하는 것을 특징으로 한다. Hidden Layer에서 입력에 대한 representation을 학습하므로 unsupervised learning이다.</p>

<p>Bengio 교수가 쓴 <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Learning Deep Architecture for AI</a>의 4.6절을 보면, <em>Autoencoder</em>에 대한 설명이 나와있는데, 필요한 내용은 Tutorial에 충분히 옮겨져 있다.</p>

<p>Autoencoder는 다음과 같은 구성을 갖고 있다.</p>

<p><img class="col two center" src="/images/201604/Autoencoder_structure.png" /></p>

<p>Theano에서는 1 hidden layered perceptron으로 구성되어 있으며 식으로 나타내면 다음과 같다.</p>

<p><strong>Encoding</strong></p>

<script type="math/tex; mode=display">y = s(\mathrm{Wx + b})</script>

<p>$s$는 비선형의 activation 함수이며, 이 예제에서는 <em>sigmoid</em>를 이용했다.</p>

<p><strong>Reconstruction</strong></p>

<script type="math/tex; mode=display">z = s(\mathrm{W'y + b'})</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">dA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="s">"""Denoising Auto-Encoder class (dA)
  """</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">numpy_rng</span><span class="p">,</span>
    <span class="n">theano_rng</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">n_visible</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">bhid</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">bvis</span><span class="o">=</span><span class="bp">None</span>
  <span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_visible</span> <span class="o">=</span> <span class="n">n_visible</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
    
    <span class="c"># Random Number Generation</span>
    <span class="c"># create a Theano random generator that gives symbolic random values</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">theano_rng</span><span class="p">:</span>
        <span class="n">theano_rng</span> <span class="o">=</span> <span class="n">RandomStreams</span><span class="p">(</span><span class="n">numpy_rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">30</span><span class="p">))</span>

    <span class="c"># note : W' was written as `W_prime` and b' as `b_prime`</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">W</span><span class="p">:</span>
        <span class="c"># W is initialized with `initial_W` which is uniformely sampled</span>
        <span class="c"># from -4*sqrt(6./(n_visible+n_hidden)) and</span>
        <span class="c"># 4*sqrt(6./(n_hidden+n_visible))the output of uniform if</span>
        <span class="c"># converted using asarray to dtype</span>
        <span class="c"># theano.config.floatX so that the code is runable on GPU</span>
        <span class="n">initial_W</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="n">numpy_rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
                <span class="n">low</span><span class="o">=-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_hidden</span> <span class="o">+</span> <span class="n">n_visible</span><span class="p">)),</span>
                <span class="n">high</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_hidden</span> <span class="o">+</span> <span class="n">n_visible</span><span class="p">)),</span>
                <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_visible</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
            <span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
        <span class="p">)</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">initial_W</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'W'</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">bvis</span><span class="p">:</span>
        <span class="n">bvis</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">n_visible</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">bhid</span><span class="p">:</span>
        <span class="n">bhid</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">n_hidden</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
            <span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span>
            <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
    <span class="c"># b corresponds to the bias of the hidden</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">bhid</span>
    <span class="c"># b_prime corresponds to the bias of the visible</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b_prime</span> <span class="o">=</span> <span class="n">bvis</span>
    <span class="c"># tied weights, therefore W_prime is W transpose</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W_prime</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">theano_rng</span> <span class="o">=</span> <span class="n">theano_rng</span>
    <span class="c"># if no input is given, generate a variable representing the input</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c"># we use a matrix because we expect a minibatch of several</span>
        <span class="c"># examples, each example being a row</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'input'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="nb">input</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_prime</span><span class="p">]</span></code></pre></figure>

<h2 id="loss-function">Loss Function</h2>

<p>autoencoder는 ANN의 출력이 입력을 복원하도록 하는 것이 특징이다. 따라서 loss 함수는 다음과 같다.</p>

<script type="math/tex; mode=display">L(\mathrm{xz}) = ||\mathrm{x - z}||^2</script>

<p>여기서 입력 데이터가 bit vector이 vectors of bit probabilities(?)라면 cross entropy를 적용하여 다음과 같이 나타낼 수 있다.</p>

<script type="math/tex; mode=display">L(\mathrm{xz}) = - \sum _{k = 1} ^d [\mathrm{x}_k \mathrm{log} \mathrm{z}_k + (1 - \mathrm{x}_k) \mathrm{log}(1 - \mathrm{z}_k)]</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span> <span class="o">...</span> <span class="k">continue</span> <span class="n">of</span> <span class="n">dA</span> <span class="k">class</span> <span class="err">... }
  </span><span class="nc">def</span> <span class="n">get_hidden_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="s">""" Computes the values of the hidden layer """</span>
    <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_reconstructed_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
    <span class="s">"""Computes the reconstructed input given the values of the
    hidden layer

    """</span>
    <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_prime</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_prime</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_cost_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corruption_level</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="s">""" This function computes the cost and the updates for one trainng
    step of the dA """</span>

    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hidden_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reconstructed_input</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c"># note : we sum over the size of a datapoint; if we are using</span>
    <span class="c">#        minibatches, L will be a vector, with one entry per</span>
    <span class="c">#        example in minibatch</span>
    <span class="n">L</span> <span class="o">=</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c"># note : L is now a vector, where each element is the</span>
    <span class="c">#        cross-entropy cost of the reconstruction of the</span>
    <span class="c">#        corresponding example of the minibatch. We need to</span>
    <span class="c">#        compute the average of all these to get the cost of</span>
    <span class="c">#        the minibatch</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

    <span class="c"># compute the gradients of the cost of the `dA` with respect</span>
    <span class="c"># to its parameters</span>
    <span class="n">gparams</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="c"># generate the list of updates</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gparam</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">gparam</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">gparams</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></code></pre></figure>

<h2 id="training">Training</h2>

<p>매 입력마다 학습을 시키면서 입력값과 출력값을 비교하여 back-propagation을 통해 학습을 시키면 된다.</p>

<p>Theano 예제에서는 stochastic gradient descdent 방식으로 back-propagation을 수행한다.</p>

<p>한편 Wikipedia에서 다룬 <a href="https://en.wikipedia.org/wiki/Autoencoder#Training">Autoencoder Training</a>에서는 pretraining(<a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machine?</a>]을 통해서 initial weight을 초기화 하여 learning을 할 경우에 learning의 성능을 높일 수 있다고 한다. 이것을 <a href="https://en.wikipedia.org/wiki/Deep_belief_network">Deep Brief Network</a>라 한다.</p>

<h2 id="tied-weights">Tied Weights</h2>

<p>Theano tutorial에서는 선택적이라고 말하면서, <em>tied weights</em>에 대해 소개하고 있다. 이는 reconstruction에서 사용될 weight matrix를 encoder의 weight matrix의 transpose 된 것이라고 간주하는 것을 말한다.</p>

<p>tied weight가 autoencoder에 가져다 주는 이점은 다음과 같다.</p>

<ol>
  <li>Learning parameter의 수 제한</li>
  <li>Regularization</li>
</ol>

<h1 id="denoising-autoencoder">Denoising Autoencoder</h1>

<p>Hidden layer로 하여금 입력 데이터의 특징들을 학습하게 하는 것이 아니라 더 robust한 시스템을 만들기 위하여 입력데이터에 noise를 섞어, 이를 학습시킨다. 따라서 denosing autoencoder는 autoencoder의 구조에서 corruption을 하기 위한 절차를 추가하는 정도면 된다.</p>

<p>Theano 예제에서는 확률적으로 masking 하는 방법으로 corruption을 하는 방법을 사용한다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span> <span class="o">...</span> <span class="k">continue</span> <span class="n">of</span> <span class="n">dA</span> <span class="n">Class</span> <span class="o">...</span> <span class="p">}</span>
  <span class="k">def</span> <span class="nf">get_corrupted_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">corruption_level</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theano_rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">p</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">corruption_level</span><span class="p">,</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span> <span class="o">*</span> <span class="nb">input</span>

  <span class="k">def</span> <span class="nf">get_cost_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corruption_level</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="s">""" This function computes the cost and the updates for one trainng
    step of the dA """</span>

    <span class="c"># Input data에 대하여 Corruption을 하여 learning에 반영되도록 한다.</span>
    <span class="n">tilde_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_corrupted_input</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">corruption_level</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hidden_values</span><span class="p">(</span><span class="n">tilde_x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reconstructed_input</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c"># note : we sum over the size of a datapoint; if we are using</span>
    <span class="c">#        minibatches, L will be a vector, with one entry per</span>
    <span class="c">#        example in minibatch</span>
    <span class="n">L</span> <span class="o">=</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c"># note : L is now a vector, where each element is the</span>
    <span class="c">#        cross-entropy cost of the reconstruction of the</span>
    <span class="c">#        corresponding example of the minibatch. We need to</span>
    <span class="c">#        compute the average of all these to get the cost of</span>
    <span class="c">#        the minibatch</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

    <span class="c"># compute the gradients of the cost of the `dA` with respect</span>
    <span class="c"># to its parameters</span>
    <span class="n">gparams</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="c"># generate the list of updates</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gparam</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">gparam</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">gparams</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></code></pre></figure>

  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20/metal/2016-04-25-theano-denoising-autoencoder/%20via%20&#64;haanjack&hashtags=DeepLearning,Unsupervised learning,Neural Networks,Theano,Auto-Encoder,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=/metal/2016-04-25-theano-denoising-autoencoder/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=/metal/2016-04-25-theano-denoising-autoencoder/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'haanjack';
        var disqus_title = '';
        var disqus_url = '/metal/2016-04-25-theano-denoising-autoencoder/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
