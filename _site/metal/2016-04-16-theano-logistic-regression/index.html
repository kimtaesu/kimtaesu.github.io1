<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Theano를 통한 Logical Regression</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/metal/2016-04-16-theano-logistic-regression/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*kimtaesu</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- kimtaesu instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/android">Android</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/kotlin">Kotlin</a>
          
        
          
        
          
          <a class="page-link" href="/python">Python</a>
          
        
          
          <a class="page-link" href="/self-improvement/">Self-Improve</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Theano를 통한 Logical Regression</h1>
    <p class="post-meta">April 15, 2016</p>
  </header>

  <article class="post-content">
    <h1 id="theano-logistic-regression-구현">Theano Logistic Regression 구현</h1>

<p>Classification의 대표적인 예제인 MNIST를 통해서 Theano에서 구현하는 Logistic Regression을 살펴보자.
내용을 따라가다보면 Machine Learning의 어떻게 녹아들어가 있는지 살펴볼 수 있다.</p>

<h2 id="라이브러리-로드">라이브러리 로드</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="n">__docformat__</span> <span class="o">=</span> <span class="s">'restructedtext en'</span>

<span class="kn">import</span> <span class="nn">six.moves.cPickle</span> <span class="kn">as</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">timeit</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span></code></pre></figure>

<p>이번 예제에서는 위와 같은 library가 사용된다.</p>
<ul>
  <li>pickle: 소스 데이터로 mnist의 pickled data를 사용한다.</li>
  <li>timeit: 함수의 동작시간을 측정하기 위해 사용된다.</li>
  <li>numpy: Theano에 데이터를 입출력하기 위해서 필요하다.</li>
  <li>teano, theano.tensor: Machine learning algorithm</li>
</ul>

<h2 id="theano를-사용한-deep-learning과-machine-learning의-차이">Theano를 사용한 Deep Learning과 Machine Learning의 차이</h2>

<h3 id="machine-learning-절차">Machine Learning 절차</h3>
<ol>
  <li>
    <p>Hypothesis Function
  <script type="math/tex">h_\theta(x) = g(\theta ^T x)</script></p>
  </li>
  <li>
    <p>Cost Function</p>
  </li>
</ol>

<script type="math/tex; mode=display">J(\theta) = \frac {1}{m} \sum _{i = 1} ^m Cost(h _\theta(x^{(i)}), y^{(i)})</script>

<ol>
  <li>
    <p>Gradient Descent &amp; Parameter Update
  각 Parameter 별로 gradient을 계산하여 Parameter를 업데이트 한다.</p>
  </li>
  <li>
    <p>Prediction</p>
    <ul>
      <li>Gradient에 의해 업데이트 된 Parameter로 Cost를 재계산 한다.</li>
      <li>모델 적용시, 수렴된 파라미터를 기반으로 새로운 데이터의 속성을 분류한다.</li>
    </ul>
  </li>
</ol>

<h3 id="deep-learning-절차">Deep Learning 절차</h3>

<ol>
  <li>Softmax regression
  Deep Learning에서는 softmax 함수를 이용하여 logistic regression을 수행한다. Softmax 함수는 여러 종류로 분류하는 것을 보다 일반화한 것이다. 출력이 pdf (probability density function)이므로, 총합은 1이며, 각 요소별 분류간 확률이 나온다.</li>
</ol>

<p><a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">Stanford Deep Learning Tutorial</a>을 보면 좀 더 자세한 수식이 나오지만, 따질려다보니 지금 다룰 만한 내용이 아닌 것 같아서 넘어갈 것이다.</p>

<p>여하튼, Deep Learning은 다음과 같이 구성한다.</p>

<script type="math/tex; mode=display">P(Y = i|x, W, b) = softmax_i(Wx+b) = \frac {e^{W_ix+b_i}}{\sum_je^{W_jx+b_j}}</script>

<script type="math/tex; mode=display">y_{pred} = argmax_iP(Y=i|x,W,b)</script>

<ol>
  <li>
    <p>Cost function
  Deep Learning도 Gradient Descent를 사용하며, Gradient 대상이 되는 Cost Funciton이 있으며, Tutorial을 보면 Machine Learning에서 사용하는 식과 동일하다.</p>
  </li>
  <li>
    <p>Gradient Descent &amp; Prameter Update
  Theano 같이 Symbolic programming을 하는 환경에서는 Gradient 연산을 symbolic으로 대체할 수 있다. 너무 좋다…</p>
  </li>
  <li>
    <p>Prediction
  Machine Learning과 동일하다.</p>
  </li>
</ol>

<h2 id="logisticregression-클래스-구현">LogisticRegression 클래스 구현</h2>

<p>Logistic Regression을 수행하기 위한 Softmax Regression에 사용할 파라미터 및 함수로 구성된 Class를 생성한다.</p>

<h3 id="파라미터-구성">파라미터 구성</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
    <span class="c">####### start-snippet-1 ######</span>
    <span class="c"># initialize with 0 the weights W as a matrix of shape (n_in, n_out)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
      <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
      <span class="p">),</span>
      <span class="n">name</span><span class="o">=</span><span class="s">'W'</span><span class="p">,</span>
      <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span>
  <span class="p">)</span>
  <span class="c"># initialize the biases b as a vector of n_out 0s</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span>
    <span class="n">value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">n_out</span><span class="p">,),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
        <span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span>
        <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span>
      <span class="p">)</span>

  <span class="c"># Classificion의 확률 분포 계산</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

  <span class="c"># softmax에 의해 계산된 확률 분포를 바탕으로 데이터 분류</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="c">####### end-snippet-1 #######</span>

  <span class="c"># parameters of the model</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

  <span class="c"># keep track of model input</span>
  <span class="bp">self</span><span class="o">.</span><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span></code></pre></figure>

<h3 id="loss-함수">Loss 함수</h3>
<p>문서에서 보면, ‘multi-class logistic regression에서 negative log-likelihood를 주로 사용한다’고 되어 있다. 이유는 안 써있다…;;</p>

<p>나는 <strong>negative log-likelihood</strong>도 모르니까 적당히 넘어가지 말고 찾아보기로 했다.
Googling을 해서 찾아본 <a href="https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/">문서</a>를 보면, 
likelihood는 조건부 확률을 의미하며 다음식과 같이 구성된다.</p>

<script type="math/tex; mode=display">\mathcal{L}(\theta\,|\,x_1,\ldots,x_n) = f(x_1,x_2,\ldots,x_n|\theta) = \prod\limits_{i=1}^n f(x_i|\theta)</script>

<p>조건부 확률에서 여러 조건을 만족시키는 경우에 대한 식이니까 길기만 할뿐 별 건 아니다. 이것에 log를 하면 다음처럼 된다.</p>

<script type="math/tex; mode=display">\log \mathcal{L}(\theta\,|\,x_1,\ldots,x_n) = \sum\limits_{i=1}^n \log f(x_i|\theta)</script>

<p>Log를 통해서 얻을 수 있는 이익은 다음과 같다.</p>

<ol>
  <li>Underflow를 방지할 수 있다</li>
  <li>곱셈을 덧셈으로 바꿔준다</li>
  <li>log는 단조함수이므로, 쉽게 transform 할 수 있다.</li>
</ol>

<p>이제 여기서 확률분포(pdf)를 생각해볼때, 가장 확률이 높은 조건을 <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">MLE(maximum likelihood estimator)</a>라고 한다.</p>

<script type="math/tex; mode=display">\hat{\theta}_{MLE} = \underset{\theta}{\arg\max} \sum\limits_{i=1}^n \log f(x_i|\theta)</script>

<p>이는 다음과 같은 관계를 충족시킨다.</p>

<p>$\underset{x}{\arg\max} (x)  = \underset{x}{\arg\min} (-x)$</p>

<p>따라서 deep learning tutorial에서 언급한 negative log-likelihood는 softmax의 결과인 확률 분포상에서 오차를 cost로 삼기 위해서 사용하는 연산인것 같다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">...</span> <span class="n">Continue</span> <span class="n">of</span> <span class="n">LogisticRegression</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_y_given_x</span><span class="p">)[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">])</span>
    </code></pre></figure>

<p>negative_log_likelihood 함수는 theano 답게 symbolic expression으로 되어 있다.</p>

<p>음.. 수학적인 부분에 대해서는 좀 더 알아보고 내용을 가다듬어야 할 것 같다.</p>

<h3 id="error">Error</h3>

<p>학습된 파라미터로 예측한 결과와 실제 값이 다른 비율을 반환한다. 이를 위해서 y에는 실제 값을 전달해준다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">...</span> <span class="n">Continue</span> <span class="n">of</span> <span class="n">LogisticRegression</span> <span class="o">...</span>

<span class="k">def</span> <span class="nf">errors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="c"># check if y has same dimension of y_pred</span>
  <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">TypeError</span><span class="p">(</span>
      <span class="s">'y should have the same shape as self.y_pred'</span><span class="p">,</span>
      <span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="nb">type</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="nb">type</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="c"># check if y is of the correct datatype</span>
  <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'int'</span><span class="p">):</span>
    <span class="c"># the T.neq operator returns a vector of 0s and 1s, where 1</span>
    <span class="c"># represents a mistake in prediction</span>
    <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">neq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span></code></pre></figure>

<h3 id="logisticregression-class의-초기화">LogisticRegression Class의 초기화</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span></code></pre></figure>

<p>MNIST의 이미지 크기가 $28 \times 28$이고, MNIST에서 사용되는 숫자의 종류는 10가지이므로 위 코드와 같이 입력한다.</p>

<h2 id="loaddata">LoadData</h2>

<p><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>를 위해서 제공되는 파일을 train/valid/test 각각 목적에 맞게 불러들인다.
MNIST의 데이터는 $28 \times 28$ 크기의 이미지이며, 손으로 쓰여진 그림이다. 각각의 이미지는 손으로 쓰여진 숫자(x)와 어떤 숫자인지(y) 알려준다.</p>

<p>좋은 점은 이미지의 크기나 이미지 구획에 대해서 고민할 필요없이 잘 정리돈 추출된 데이터 집합이므로, 데이터 엔지니어에 의해서 데이터가 정리된다든지 하는 작업을 건너뛰고 바로 Classification을 해볼 수 있다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
  <span class="c"># Download the MNIST dataset if it is not present</span>
  <span class="c"># MNIST 파일이 없는경우 다운을 받아준다. 코드에 있는 url 주소를 참조하여 직접 다운로드 받는다.</span>
  <span class="n">data_dir</span><span class="p">,</span> <span class="n">data_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">data_dir</span> <span class="o">==</span> <span class="s">""</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
      <span class="c"># Check if dataset is in the data directory.</span>
      <span class="n">new_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
          <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">__file__</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
          <span class="s">".."</span><span class="p">,</span>
          <span class="s">"data"</span><span class="p">,</span>
          <span class="n">dataset</span>
      <span class="p">)</span>
      <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">new_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">data_file</span> <span class="o">==</span> <span class="s">'mnist.pkl.gz'</span><span class="p">:</span>
          <span class="n">dataset</span> <span class="o">=</span> <span class="n">new_path</span>

  <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span> <span class="ow">and</span> <span class="n">data_file</span> <span class="o">==</span> <span class="s">'mnist.pkl.gz'</span><span class="p">:</span>
      <span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">urllib</span>
      <span class="n">origin</span> <span class="o">=</span> <span class="p">(</span>
          <span class="s">'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'</span>
      <span class="p">)</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Downloading data from </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">origin</span><span class="p">)</span>
      <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">origin</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'... loading data'</span><span class="p">)</span>

  <span class="c"># Load the dataset</span>
  <span class="c"># 압축된 파일을 풀고, pickle 되어 있는 파일을 읽어들인다.</span>
  <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
          <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin1'</span><span class="p">)</span>
      <span class="k">except</span><span class="p">:</span>
          <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

  <span class="c"># GPU(CUDA) 메모리를 사용하기 위해 theano shared 메모리로 변환시켜주는 함수</span>
  <span class="k">def</span> <span class="nf">shared_dataset</span><span class="p">(</span><span class="n">data_xy</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
      <span class="s">""" Function that loads the dataset into shared variables

      The reason we store our dataset in shared variables is to allow
      Theano to copy it into the GPU memory (when code is run on GPU).
      Since copying data into the GPU is slow, copying a minibatch everytime
      is needed (the default behaviour if the data is not in a shared
      variable) would lead to a large decrease in performance.
      """</span>
      <span class="n">data_x</span><span class="p">,</span> <span class="n">data_y</span> <span class="o">=</span> <span class="n">data_xy</span>
      <span class="n">shared_x</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_x</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                               <span class="n">borrow</span><span class="o">=</span><span class="n">borrow</span><span class="p">)</span>
      <span class="n">shared_y</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_y</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span>
                               <span class="n">borrow</span><span class="o">=</span><span class="n">borrow</span><span class="p">)</span>
      <span class="c"># When storing data on the GPU it has to be stored as floats</span>
      <span class="c"># therefore we will store the labels as ``floatX`` as well</span>
      <span class="c"># (``shared_y`` does exactly that). But during our computations</span>
      <span class="c"># we need them as ints (we use labels as index, and if they are</span>
      <span class="c"># floats it doesn't make sense) therefore instead of returning</span>
      <span class="c"># ``shared_y`` we will have to cast it to int. This little hack</span>
      <span class="c"># lets ous get around this issue</span>
      <span class="k">return</span> <span class="n">shared_x</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">shared_y</span><span class="p">,</span> <span class="s">'int32'</span><span class="p">)</span>

  <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
  <span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">valid_set</span><span class="p">)</span>
  <span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span> <span class="o">=</span> <span class="n">shared_dataset</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

  <span class="n">rval</span> <span class="o">=</span> <span class="p">[(</span><span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">),</span> <span class="p">(</span><span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span><span class="p">),</span>
          <span class="p">(</span><span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">rval</span></code></pre></figure>

<h2 id="learning-model">Learning Model</h2>

<p>이제 gradient descendent를 이용하여, Learning Parameter를 Tunning 한다.
여기서 Theano의 가장 큰 특징은, 다른 언어를 사용해서 개발을 할 때는 직접 gradient descedent 식을 코드로 작성해서 사용해야하지만, Theano는 <strong>grad</strong> 함수에 cost function 만을 전달해주면 된다. 즉, gradient에 대해 구현할 필요가 없이 다음과 같이 code를 작성만 해주면 된다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">g_W</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
<span class="n">g_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">)</span></code></pre></figure>

<p>계산된 gradient는 learning parameter에 적용되야 하며 어떻게 모델에 적용될 것인지 명세를 해주어야 한다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># specify how to update the parameters of the model as a list of</span>
<span class="c"># (variable, update expression) pairs.</span>
<span class="n">updates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_W</span><span class="p">),</span>
           <span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_b</span><span class="p">)]</span></code></pre></figure>

<p>이제 실질적으로 learning을 수행하기 위한 <strong>train_model</strong> 함수를 작성한다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
    <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
    <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span></code></pre></figure>

<p>theano의 함수를 작성하는 방법이란게, 이런식으로 필요한 동작들을 parameter로 전달해 주는 것이다. theano function은 symbolic description으로 명세된 코드들이 인자로 전달되었을때 실제 code로 컴파일 되어 실제 동작할 수 있도록 해준다. python programming 처럼 parameter를 지정할 수 있으며, 이중에서 input과 output만을 제외하면 있어도 되고 없어도 된다.</p>

<p>train_model 함수를 구성하기 위해서는 다음과 같은 것들을 해야한다.</p>

<ol>
  <li>gradient 연산 및 learning parameter 업데이트</li>
  <li>batch gradient를 수행하기 위한 동작</li>
</ol>

<p>앞서 작성한 updates의 명세를 보면, cost의 gradient와 learning parameter인 W, b가 계산된다. 또한 cost의 gradient를 계산하기 위한 cost function을 계산하기 위해서는 입력데이터인 x, y가 주어져야 한다. 이때 이 예제에서는 batch gradient descent를 이용하므로, batch size 단위로 데이터를 입력할 수 있어야 하며, 이를 위해 train_model 함수는  batch index를 입력 파라미터로 사용한다.</p>

<h2 id="test-model">Test Model</h2>

<p>learning parameter를 업데이트할 수 있는 것이 train_model 함수라면, 테스트 데이터를 바탕으로 예측된 값과 얼마나 차이가 나는지 확인 하는 것이 test 함수이다.
이때 얼마나 차이가 나는지 확인하기 위해서 앞서 생성한 LogisticRegression 클래스의 error 함수를 이용한다.</p>

<p>validation과 test는 데이터 셋이 다를 뿐 같은 동작을 하므로 다음과 같이 코드를 작성한다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
    <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
    <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
    <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span></code></pre></figure>

<p>동작 방식은 앞서 <strong>train_model</strong> 함수와 동일하게 입력으로 주어진 batch index를 기준으로 주어진 데이터 셋에서 적절한 data를 입력받아 error값, 즉 학습된 파라미터를 바탕으로 얼마나 실제 값과 예측 값이 차이가 나는 비율을 확인한다.</p>

<h2 id="processing">Processing</h2>

<p>이제 만든 함수들을 하나로 묶어보자.</p>

<h3 id="loading-data">Loading Data</h3>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sgd_optimization_mnist</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                           <span class="n">dataset</span><span class="o">=</span><span class="s">'mnist.pkl.gz'</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">600</span><span class="p">):</span>
  <span class="n">datasets</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

  <span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

  <span class="c"># compute number of minibatches for training, validation and testing</span>
  <span class="n">n_train_batches</span> <span class="o">=</span> <span class="n">train_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
  <span class="n">n_valid_batches</span> <span class="o">=</span> <span class="n">valid_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>
  <span class="n">n_test_batches</span> <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span></code></pre></figure>

<h3 id="build-model">build model</h3>

<p>지금까지 설명한 부분들을 코드로 작성한다. 입력에 사용될 parameter들에 대하여(batch index, x, y)에 대하여 사용할 수 있도록 theano의 변수로 정의해준다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span> <span class="o">...</span> <span class="k">continue</span> <span class="n">of</span> <span class="n">sgd_optimization_mnist</span> <span class="o">...</span> <span class="p">}</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'... building the model'</span><span class="p">)</span>

  <span class="c"># allocate symbolic variables for the data</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span>  <span class="c"># index to a [mini]batch</span>

  <span class="c"># generate symbolic variables for input (x and y represent a</span>
  <span class="c"># minibatch)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>  <span class="c"># data, presented as rasterized images</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>  <span class="c"># labels, presented as 1D vector of [int] labels</span>

  <span class="c"># construct the logistic regression class</span>
  <span class="c"># Each MNIST image has size 28*28</span>
  <span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

  <span class="c"># the cost we minimize during training is the negative log likelihood of</span>
  <span class="c"># the model in symbolic format</span>
  <span class="n">cost</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

  <span class="c"># compiling a Theano function that computes the mistakes that are made by</span>
  <span class="c"># the model on a minibatch</span>
  <span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>

  <span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>

  <span class="c"># compute the gradient of cost with respect to theta = (W,b)</span>
  <span class="n">g_W</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
  <span class="n">g_b</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span> <span class="n">wrt</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

  <span class="c"># start-snippet-3</span>
  <span class="c"># specify how to update the parameters of the model as a list of</span>
  <span class="c"># (variable, update expression) pairs.</span>
  <span class="n">updates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">classifier</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_W</span><span class="p">),</span>
             <span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g_b</span><span class="p">)]</span>

  <span class="c"># compiling a Theano function `train_model` that returns the cost, but in</span>
  <span class="c"># the same time updates the parameter of the model based on the rules</span>
  <span class="c"># defined in `updates`</span>
  <span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
      <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
      <span class="n">outputs</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
      <span class="n">updates</span><span class="o">=</span><span class="n">updates</span><span class="p">,</span>
      <span class="n">givens</span><span class="o">=</span><span class="p">{</span>
          <span class="n">x</span><span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">],</span>
          <span class="n">y</span><span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]</span>
      <span class="p">}</span>
  <span class="p">)</span>
  <span class="c"># end-snippet-3</span></code></pre></figure>

<h3 id="train-model">TRAIN MODEL</h3>

<p>이제 loop을 돌면서 보다 learning parameter를 update한다. 그리고 validation_frequency로 계산된 주기에 맞춰서 validation 코드를 실행한다.</p>

<p>이 예제에서는 원하는 조건이 달성 되었을 경우 학습을 종료할 수 있는 early termination에 대한 코드도 추가되어 있다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span> <span class="o">...</span> <span class="k">continue</span> <span class="n">of</span> <span class="n">sgd_optimization_mnist</span> <span class="o">...</span> <span class="p">}</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'... training the model'</span><span class="p">)</span>
  <span class="c"># early-stopping parameters</span>
  <span class="n">patience</span> <span class="o">=</span> <span class="mi">5000</span>  <span class="c"># look as this many examples regardless</span>
  <span class="n">patience_increase</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c"># wait this much longer when a new best is</span>
                                <span class="c"># found</span>
  <span class="n">improvement_threshold</span> <span class="o">=</span> <span class="mf">0.995</span>  <span class="c"># a relative improvement of this much is</span>
                                <span class="c"># considered significant</span>
  <span class="n">validation_frequency</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">,</span> <span class="n">patience</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
                                <span class="c"># go through this many</span>
                                <span class="c"># minibatche before checking the network</span>
                                <span class="c"># on the validation set; in this case we</span>
                                <span class="c"># check every epoch</span>

  <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">inf</span>
  <span class="n">test_score</span> <span class="o">=</span> <span class="mf">0.</span>
  <span class="n">start_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>

  <span class="n">done_looping</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">n_epochs</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done_looping</span><span class="p">):</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">minibatch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">):</span>
      <span class="n">minibatch_avg_cost</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">minibatch_index</span><span class="p">)</span>
      <span class="c"># iteration number</span>
      <span class="nb">iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_train_batches</span> <span class="o">+</span> <span class="n">minibatch_index</span>

      <span class="k">if</span> <span class="p">(</span><span class="nb">iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">validation_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c"># compute zero-one loss on validation set</span>
        <span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">validate_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_valid_batches</span><span class="p">)]</span>
        <span class="n">this_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">validation_losses</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span>
            <span class="s">'epoch </span><span class="si">%</span><span class="s">i, minibatch </span><span class="si">%</span><span class="s">i/</span><span class="si">%</span><span class="s">i, validation error </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span> <span class="o">%</span>
            <span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span>
                <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">n_train_batches</span><span class="p">,</span>
                <span class="n">this_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="c"># if we got the best validation score until now</span>
        <span class="k">if</span> <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span><span class="p">:</span>
          <span class="c">#improve patience if loss improvement is good enough</span>
          <span class="k">if</span> <span class="n">this_validation_loss</span> <span class="o">&lt;</span> <span class="n">best_validation_loss</span> <span class="o">*</span>  \
             <span class="n">improvement_threshold</span><span class="p">:</span>
              <span class="n">patience</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">*</span> <span class="n">patience_increase</span><span class="p">)</span>

          <span class="n">best_validation_loss</span> <span class="o">=</span> <span class="n">this_validation_loss</span>
          <span class="c"># test it on the test set</span>

          <span class="n">test_losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_model</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_batches</span><span class="p">)]</span>
          <span class="n">test_score</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_losses</span><span class="p">)</span>

          <span class="k">print</span><span class="p">(</span>
              <span class="p">(</span>
                  <span class="s">'     epoch </span><span class="si">%</span><span class="s">i, minibatch </span><span class="si">%</span><span class="s">i/</span><span class="si">%</span><span class="s">i, test error of'</span>
                  <span class="s">' best model </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span>
              <span class="p">)</span> <span class="o">%</span>
              <span class="p">(</span>
                  <span class="n">epoch</span><span class="p">,</span>
                  <span class="n">minibatch_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="n">n_train_batches</span><span class="p">,</span>
                  <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span>
              <span class="p">)</span>
          <span class="p">)</span>

          <span class="c"># save the best model</span>
          <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'best_model.pkl'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
              <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">patience</span> <span class="o">&lt;=</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">done_looping</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">break</span>

  <span class="n">end_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span>
      <span class="p">(</span>
          <span class="s">'Optimization complete with best validation score of </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">,'</span>
          <span class="s">'with test performance </span><span class="si">%</span><span class="s">f </span><span class="si">%%</span><span class="s">'</span>
      <span class="p">)</span>
      <span class="o">%</span> <span class="p">(</span><span class="n">best_validation_loss</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">,</span> <span class="n">test_score</span> <span class="o">*</span> <span class="mf">100.</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'The code run for </span><span class="si">%</span><span class="s">d epochs, with </span><span class="si">%</span><span class="s">f epochs/sec'</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">epoch</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">*</span> <span class="n">epoch</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)))</span>
  <span class="k">print</span><span class="p">((</span><span class="s">'The code for file '</span> <span class="o">+</span>
         <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">__file__</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
         <span class="s">' ran for </span><span class="si">%.1</span><span class="s">fs'</span> <span class="o">%</span> <span class="p">((</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))),</span> <span class="nb">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">)</span></code></pre></figure>


  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://localhost:4000/metal/2016-04-16-theano-logistic-regression/%20via%20&#64;&hashtags=DeepLearning,Classification,Theano,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/metal/2016-04-16-theano-logistic-regression/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://localhost:4000/metal/2016-04-16-theano-logistic-regression/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = '';
        var disqus_title = '';
        var disqus_url = '/metal/2016-04-16-theano-logistic-regression/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
