<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Linear Regression - II</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="/metal/2016-04-06-ml-ng-linear-regression-extra/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <line rel="stylesheet" href="/css/solarized_light.css"
    
    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>


  <body>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol><symbol id="icon-linkedin" viewBox="0 0 1792 1792"><path class="path1" d="M477 625v991h-330v-991h330zm21-306q1 73-50.5 122t-135.5 49h-2q-82 0-132-49t-50-122q0-74 51.5-122.5t134.5-48.5 133 48.5 51 122.5zm1166 729v568h-329v-530q0-105-40.5-164.5t-126.5-59.5q-63 0-105.5 34.5t-63.5 85.5q-11 30-11 81v553h-329q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5 114.5-15.5q171 0 275 113.5t104 332.5z"/></symbol></defs></svg>
    <header class="site-header">

  <div class="wrapper">
    <div class="site-title" >*hanjack</div>
      
    <nav class="site-nav">
        
      <div class="trigger">
        <!-- hanjack instead of blog -->
        <a class="page-link" href="/">blog</a>

        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
          <a class="page-link" href="/cuda/">CUDA</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/metal/">Machine Learning</a>
          
        
          
          <a class="page-link" href="/portfolio/">Gallery</a>
          
        

      </div>
    </nav>
  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Linear Regression - II</h1>
    <p class="post-meta">April 6, 2016</p>
  </header>

  <article class="post-content">
    <p>강의에서 다뤄진 Linear Regression을 하는데 있어서 고려해야하는 내용은 다음과 같다.</p>

<ol>
  <li><a href="feature scaling">Feature Scaling</a></li>
  <li><a href="learning rate">Learning Rate</a></li>
  <li><a href="f p regression">Features and Polynomial Regression</a></li>
  <li><a href="normal equation">Normal Equation</a></li>
</ol>

<p><a class="anckor" id="feature scaling"></a></p>

<h1 id="feature-scaling">Feature Scaling</h1>

<p>여러개의 Feature가 있을 때, 유사한 Scale을 갖도록 Feature Normalization을 해주어야 한다. 
만약 $\theta _1$이 $\theta _2$에 비해서 매우 크다면 아래 그림과 같이 일그러진 형태의 Cost 평면을 형성하게 된다. 이는 수렴하는 방향에도 영향을 주어, global minimum 값을 찾는 시간을 지연시키게 된다.
<img class="col one center" src="/images/201604/2F86C27F-0847-49DA-961E-64CECA157187.png" /></p>

<p>그렇다면 적절한 Normalization 값은 어떤 값이어야 하는가.
Mean Normalization을 통해 Feature Scaling을 하는 것을 권한다. Mean Normalization을 했을때 장점은, Scaling 후의 Feautre의 평균값이 0이 되기 때문이다.</p>

<p>Mean Normalization이라고 하더라도 다음과 같이 몇가지 방법이 있을 수 있다.</p>

<ol>
  <li>최대값 이용: $ X _i \leftarrow \frac {X _i - \mu} {X _max}</li>
  <li>범위 이용: $ X _i \leftarrow \frac {X _i - \mu _i} {X _max - X _min}$</li>
  <li>분산 이용: $ X _i \leftarrow \frac {X _i - \mu _i} {s _i}$</li>
</ol>

<p><a class="anckor" id="learning rate"></a></p>

<h1 id="learning-rate-alpha">Learning Rate $\alpha$</h1>

<p>Gradient Descent 식에서 보면,
<script type="math/tex">\theta _j = \theta _j + \alpha \frac {\delta} {\delta \theta _j} J(\theta _0, \theta _1)</script> for $j = 0, j = 1$$
$\alpha$ 값은 <strong>learning rate</strong>로서 gradient 값을 반영하는 인자로서, 값의 크기에 따라 Cost Function의 Global minimum을 찾는데 영향을 미치게 된다.</p>

<ol>
  <li>$\alpha$가 너무 큰 경우
    <ul>
      <li>global minimum을 찾지 못하게 된다.</li>
    </ul>
  </li>
  <li>$\alpha$가 너무 작은 경우
    <ul>
      <li>global minimum을 향해 가겠지만, iteration이 증가하여 learning time을 증가시킨다.</li>
    </ul>
  </li>
  <li>$\alpha$가 적당한 경우
    <ul>
      <li>적절한 시점에 learning을 종료할 수 있다.</li>
    </ul>
  </li>
</ol>

<p>따라서, $\alpha$의 후보를 두고, 값을 바꿔가면서 수렴하는 정도를 살펴서 적절한 크기의 $\alpha$ 값을 선택해야 한다. 적당한 $\alpha$의 예로, 대략 3배씩 증가하는 값들을 예로 들 수 있다.
* 0.001, 0.003, 0.01, 0.03, 0.1, 0.3</p>

<p><a class="anckor" id="f p regression"></a></p>

<h1 id="features-and-polynomial-regression">Features and Polynomial Regression</h1>

<h2 id="features">Features</h2>
<p>learning에 유의미한 feature들을 선택한다. 또한 필요에 따라서 feature를 추가할수도 있다.</p>

<h2 id="polynomial-regression">Polynomial Regression</h2>

<p>polynomial regresion을 사용하게 되면 선을 이용하는 것보다 더 유연하게 데이터에 더욱 가깝게 curve fitting을 할 수 있다. 데이터의 분포에 따라서 feature의 차수에 대한 힌트를 얻을 수 있다. 아래 도표와 같이 집의 크기와 가격에 대하여 빨간x와 같이 데이터가 분포해 있다고 해보자.</p>

<p><img class="col two center" src="/images/201604/A06414E3-6BE0-44E3-A0C4-215C294B3477.png" /></p>

<p>위 데이터에 대해서 일직선 형태의 linear regression을 적용하기 보다는 곡선 형태의 선으로 regression을 하는 것이 보다 정확한 분석이 될 것이다. 다만 size에 대한 해석을 하게 되므로 이 경우에는 1차 함수를 이용하기 보다는 size(feature)에 대한 polynoial equation을 수립하여 linear regression을 수행한다.</p>

<p>주어진 데이터에 잘 맞는 곡선은 <strong>파란 선</strong>이겠지만, 좀더 큰 크기에 대한 데이터가 더 주어진다면 곡선이 어떻게 변화할 것인지 추가로 정보를 얻을 수 있다. 저렴해질 수도(점선; 그럴리 없겠지만…),  가격이 더 커질수도(녹색) 있다. feature의 차수르 바꾸어 적용한다면 더 정확한 학습이 가능하다.</p>

<p><a class="anckor" href="normal equation"></a></p>

<h1 id="normal-equation">Normal Equation</h1>

<p>Learning parameter $\theta$를 구하기 위해서 대표적으로 사용되는 것이 Gradient Descent이다. Coursera 강좌에서는 이것 되에도 직접적으로 $\theta$를 계산할 수 있는 방법으로 <strong>Normal Equation</strong>을 제시하고 있다.</p>

<p>이를테면, Gradient Descent는 Cost function의 gradient를 계속해서 추적해가면서 cost가 최소인 parameter를 찾는 방법이지만, Matrix 연산만으로도 $\theta$를 찾을 수 있는 것이다.</p>

<script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y</script>

<p>이 방법이 Gradient Descent와 비교하여 갖는 장단점은 다음과 같은 것들이 있다.</p>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>learning rate를 정해야 한다.</td>
      <td>learning rate를 선택하지 않아도 된다.</td>
    </tr>
    <tr>
      <td>iteration을 여러번 수행해야 한다.</td>
      <td>iteration이 필요하지 않으므로, 수렴여부를 확인할 필요가 없다.</td>
    </tr>
    <tr>
      <td>데이터가 많아도 잘 동작한다.</td>
      <td>데이터가 많을 경우 매우 느리다</td>
    </tr>
    <tr>
      <td> </td>
      <td>Matrix Inversion을 해야한다. 특히 $(X^T X)^{-1}$연산은 $O(n^3)$이므로 연산량이 많은 방법이다</td>
    </tr>
  </tbody>
</table>

<p>특히 샘플의 양이 많고 적음의 기준에 대해서, 1000 정도는 작은 데이터 수이고 10000개 정도면 Normal Equation을 적용하기엔 큰 데이터라고 한다.</p>

<p>또한 여기서 주의해야할 사항은 X가 Inverse가 가능해야 한다는 점이다. pseudo inverse를 이용해서 대부분 inverse를 할 수 있지만, 그래도 inverse가 가능하지 않은 singular matrix라면 다음 두가지 경우에 해당한다.</p>

<ul>
  <li>feature 중복
    <ul>
      <li>내용이 같은 feature가 포함되어 있을 수 있다.</li>
    </ul>
  </li>
  <li>너무 많은 featuer
    <ul>
      <li>feature에 비하여 데이터의 수가 적을 수 있다.</li>
      <li>feature를 줄이거나 regularization을 한다.</li>
    </ul>
  </li>
</ul>


  </article>

  <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20/metal/2016-04-06-ml-ng-linear-regression-extra/%20via%20&#64;haanjack&hashtags=Machine Learning,Linear Regression,Coursera,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=/metal/2016-04-06-ml-ng-linear-regression-extra/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=/metal/2016-04-06-ml-ng-linear-regression-extra/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>
  <section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'haanjack';
        var disqus_title = '';
        var disqus_url = '/metal/2016-04-06-ml-ng-linear-regression-extra/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p>Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.  Theme from <a href="https://github.com/bogoli/-folio" target="_blank">folio</a> by Lia Bogoev, MIT Licence. &#169; 2016.</p>
  </div>

</footer>


  </body>

</html>
